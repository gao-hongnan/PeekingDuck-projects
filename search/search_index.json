{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PeekingDuck Exercise Counter by Hongnan Gao 1st May, 2022 Introduction PeekingDuck is an open-source, modular framework in Python, built for Computer Vision (CV) inference. The name \"PeekingDuck\" is a play on: \"Peeking\" in a nod to CV; and \"Duck\" in duck typing. - Extracted from PeekingDuck . This project uses the PeekingDuck framework to create two use cases: Exercise Counter : we will create custom node(s) using the PeekingDuck Framework to count the number of times a user has performed a push-up. The example can be extended to other exercises such as sit-ups, pull-ups and other general exercises by altering the counting logic . Melanoma Prediction with Grad-CAM : we will create custom node(s) using the PeekingDuck Framework to predict the presence of melanoma in an image and output the image with Grad-CAM highlights. Installation Install the Exercise Counter using the following command: $ git clone # clone or download the repo to your working dir $ pip install -e . # install the dependencies $ cd custom_hn_exercise_counter # change to the project directory $ peekingduck run # run the project Note that this is only tested on Ubuntu-latest and Windows-latest with python version 3.8/3.9 through GitHub Actions. For a more detailed workflow, see the Workflows section. Tutorials The tutorials below walkthrough how to use the PeekingDuck framework to create custom nodes for the two use cases. PeekingDuck Exercise Counter PeekingDuck Grad-CAM Gallery Push-up Counter Demo Seeing push-up counter in action: person doing a total of \\(7\\) push-ups - photo taken by me. The counter is displayed in yellow font on the top left corner of the video/gif. Below is a download link for the push-up counter demo. There are two demos, one is the one above, and the other is a video from YouTube , uploaded by user Pedro Neto. Download Push Up Demo Zip Melanoma Grad-CAM Demo The demo of Melanoma Grad-CAM demo is shown below: First, the original predicted images are displayed: The below will be the ones with Grad-CAM: References This project took references from the Official PeekingDuck Tutorials .","title":"Home"},{"location":"#introduction","text":"PeekingDuck is an open-source, modular framework in Python, built for Computer Vision (CV) inference. The name \"PeekingDuck\" is a play on: \"Peeking\" in a nod to CV; and \"Duck\" in duck typing. - Extracted from PeekingDuck . This project uses the PeekingDuck framework to create two use cases: Exercise Counter : we will create custom node(s) using the PeekingDuck Framework to count the number of times a user has performed a push-up. The example can be extended to other exercises such as sit-ups, pull-ups and other general exercises by altering the counting logic . Melanoma Prediction with Grad-CAM : we will create custom node(s) using the PeekingDuck Framework to predict the presence of melanoma in an image and output the image with Grad-CAM highlights.","title":"Introduction"},{"location":"#installation","text":"Install the Exercise Counter using the following command: $ git clone # clone or download the repo to your working dir $ pip install -e . # install the dependencies $ cd custom_hn_exercise_counter # change to the project directory $ peekingduck run # run the project Note that this is only tested on Ubuntu-latest and Windows-latest with python version 3.8/3.9 through GitHub Actions. For a more detailed workflow, see the Workflows section.","title":"Installation"},{"location":"#tutorials","text":"The tutorials below walkthrough how to use the PeekingDuck framework to create custom nodes for the two use cases. PeekingDuck Exercise Counter PeekingDuck Grad-CAM","title":"Tutorials"},{"location":"#gallery","text":"","title":"Gallery"},{"location":"#push-up-counter-demo","text":"Seeing push-up counter in action: person doing a total of \\(7\\) push-ups - photo taken by me. The counter is displayed in yellow font on the top left corner of the video/gif. Below is a download link for the push-up counter demo. There are two demos, one is the one above, and the other is a video from YouTube , uploaded by user Pedro Neto. Download Push Up Demo Zip","title":"Push-up Counter Demo"},{"location":"#melanoma-grad-cam-demo","text":"The demo of Melanoma Grad-CAM demo is shown below: First, the original predicted images are displayed: The below will be the ones with Grad-CAM:","title":"Melanoma Grad-CAM Demo"},{"location":"#references","text":"This project took references from the Official PeekingDuck Tutorials .","title":"References"},{"location":"exercise_counter/","text":"PeekingDuck Exercise Counter by Hongnan Gao 1st May, 2022 Push-Up Counter The aim of this project is to build an exercise counter that can be used to detect push-ups, and more generic exercises in future iterations. Some content below are referenced from PeekingDuck's Tutorial . The main model is MoveNet , which outputs seventeen keypoints for the person corresponding to the different body parts as documented here 1 . Each keypoint is a pair of \\((x, y)\\) coordinates, where \\(x\\) and \\(y\\) are real numbers ranging from \\(0.0\\) to \\(1.0\\) (using relative coordinates 2 ). Note This is a Minimum Viable Product (MVP) and uses simple logic to detect push-ups. Consequently, there are a few somewhat rigid assumptions that will be made in the code. We will detail the logic in the following sections. Assumptions This project makes a few assumptions about the input data for simplicity of implementation: The user's left body parts are visible through the webcam/video and in particular, the left elbow, wrist and shoulder are crucial for our push-up counter. We can improve on this rigid requirement in the future by checking both the left and right body parts, and take the side with higher keypoints confidence . There should be only \\(1\\) person in the video. As we are using MoveNet's singlepose_thunder model, we need to impose the restriction that there should be only \\(1\\) person in the video to avoid performance issues. We can improve on this rigid requirement in the future by incorporating multi-person logic. The multipose_lightning can detect up to \\(6\\) people in the video. If user leaves the video and come back, we assume that the user is the same person and continue counting. Custom Node General Workflow Step 1. Initialize PeekingDuck Template After setting up from the Workflows section, we can start using the PeekingDuck interface. We initialize a new PeekingDuck project using the following commands: Terminal Session Initializing PeekingDuck 1 2 3 $ mkdir custom_hn_exercise_counter $ cd custom_hn_exercise_counter $ peekingduck init [ Line 1 ] : Create a new directory named custom_hn_exercise_counter for the project. [ Line 2 ] : Change to the newly created directory. [ Line 3 ] : Initialize the PeekingDuck project in the current directory with default file/folder below. Upon initialization of the project, PeekingDuck creates the following files in your new project directory: pipeline_config.yml - Contains the pipeline 3 configuration and; src/ - Folder for custom nodes. The custom_hn_exercise_counter directory currently looks like this: Directory Tree of custom_hn_exercise_counter custom_hn_exercise_counter/ \u251c\u2500\u2500 pipeline_config.yml \u2514\u2500\u2500 src/ \u2514\u2500\u2500 custom_nodes/ \u2514\u2500\u2500configs/ Step 2. Use Pipeline Recipe to Create Custom Nodes PeekingDuck provides several node types out of the box, for example a MoveNet node to detect human poses within an image. To implement additional functionality not provided by built-in nodes, we can create custom nodes with our own logic written in Python. For our project, we need to create two custom nodes. First, we will create a custom_nodes.input.visual node. Although PeekingDuck provides a visual input node, a small issue prevents filenames from URL visual sources from registering properly. Registering the filename properly is important, because we use it in later stages of our pipeline, including writing output data to CSV. To fix this issue, we implement this custom node. Additionally, we implement a custom_nodes.dabble.exercise_counter node. This node will take the keypoints from the MoveNet model and count the number of push-ups performed by the person in the input video. To create these custom nodes, we first edit our pipeline_config.yml : Show/Hide content for pipeline_config.yml pipeline_config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 nodes : - custom_nodes.input.visual : source : https://storage.googleapis.com/reighns/peekingduck/videos/push_ups.mp4 - model.yolo : model_type : \"v4tiny\" iou_threshold : 0.1 score_threshold : 0.1 detect_ids : [ \"person\" ] # [0] num_classes : 1 #- custom_nodes.dabble.debug_yolo - model.movenet : model_type : \"singlepose_thunder\" resolution : { height : 256 , width : 256 } bbox_score_threshold : 0.05 keypoint_score_threshold : 0.05 - dabble.fps - custom_nodes.dabble.exercise_counter : keypoint_threshold : 0.3 exercise_name : \"push_ups\" push_up_pose_params : { starting_elbow_angle : 155 , ending_elbow_angle : 90 , } - draw.poses - draw.legend : show : [ \"fps\" ] - output.csv_writer : stats_to_track : [ \"keypoints\" , \"bboxes\" , \"bbox_labels\" , \"num_push_ups\" , \"frame_count\" , \"expected_pose\" , \"elbow_angle\" , \"shoulder_keypoint\" , \"elbow_keypoint\" , \"wrist_keypoint\" , \"filename\" ] file_path : \"./stores/artifacts/push_ups_output_movenet.csv\" logging_interval : 0 - output.screen Adding our custom nodes (highlighted) is as simple as adding entries for them to the pipeline_config.yml file. Additionally, we add configuration for some nodes. For example, I specify that I want to use v4tiny model for the model.yolo node with a \\(0.1\\) threshold for both iou and bounding box confidence score. These configurations will then be passed to the config parameter of the model.yolo node. We then create the custom nodes using the Pipeline Recipe method 4 with the following command: Terminal Session Creating Custom Nodes $ peekingduck create-node --config_path pipeline_config.yml This will create all the nodes listed in the pipeline_config.yml file. If one decides to add more custom nodes, we can simply add them to the pipeline_config.yml file and run the command again. The updated custom_hn_exercise_counter directory currently looks like this: Directory Tree of custom_hn_exercise_counter custom_hn_exercise_counter/ \u251c\u2500\u2500 pipeline_config.yml \u2514\u2500\u2500 src/ \u2514\u2500\u2500 custom_nodes/ \u251c\u2500\u2500 configs/ | \u251c\u2500\u2500 dabble/ | | \u2514\u2500\u2500 exercise_counter.yml | \u2514\u2500\u2500 input/ | \u2514\u2500\u2500 visual.yml \u251c\u2500\u2500 dabble/ | \u2514\u2500\u2500 exercise_counter.py \u2514\u2500\u2500 input/ \u2514\u2500\u2500 visual.py custom_hn_exercise_counter now contains five files that we need to modify in order to implement our custom push-up counter. pipeline_config.yml src/custom_nodes/dabble/exercise_counter.py src/custom_nodes/configs/dabble/exercise_counter.yml src/custom_nodes/input/visual.py src/custom_nodes/configs/input/visual.py We will go into details in the next section . We will also create an additional folder stores in the same level as src . For now, we will add a folder artifacts in the stores folder, this is where we will store the output files and model artifacts. Directory Tree of custom_hn_exercise_counter custom_hn_exercise_counter/ \u251c\u2500\u2500 src/ \u251c\u2500\u2500 stores/ \u2502 \u2514\u2500\u2500 artifacts/ \u2514\u2500\u2500 pipeline_config.yml Step 3. Deep Dive into the Custom Nodes After Step 2 , three folders config , dabble and input will be populated in src . The config folder holds the configurations for the custom nodes while the dabble and input folders holds the code for the custom nodes. Info Something worth noting is that other default nodes that are in PeekingDuck will not be included in the config folder. For example, the model.yolo node is not included in the config folder because it is a default node . This is because the configurations for the default nodes are already included in the pipeline_config.yml file and will be instantiated when peekingduck run is called. custom_nodes.input.visual input/visual.yml When defining a custom node, we must provide a default configuration. We can use the default configuration from the built-in input.visual node: Show/Hide content for input.visual.yml input.visual.yml input : [ \"none\" ] output : [ \"img\" , \"filename\" , \"pipeline_end\" , \"saved_video_fps\" ] filename : video.mp4 frames_log_freq : 100 mirror_image : False resize : { do_resizing : False , width : 1280 , height : 720 } saved_video_fps : 10 source : https://storage.googleapis.com/peekingduck/videos/wave.mp4 threading : False buffering : False input/visual.py In PeekingDuck version 1.2.0 , if you define source to be a URL , the filename is not overwritten by the source filename and maintains the default video.mp4 . There is a pull request to fix a similar issue where the filename was not set if the source is a single image. In our custom_nodes.input.visual node, we apply a similar fix for the URL case. This change will help us save the filename parameter in the output CSV file later. Standing on the shoulder of giants, I quickly modified a few lines to suit my purpose as I want to save the filename parameter in my output csv file later. Therefore, the code inside this file is mostly the same except for the highlighted lines below. To implement this custom node, copy the peekingduck/pipeline/nodes/input/visual.py file from the PeekingDuck dev branch to custom_hn_exercise_counter/src/custom_nodes/input/visual.py and update the highlighted lines: input.visual.py: _determine_source_type() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def _determine_source_type ( self ) -> None : \"\"\" Determine which one of the following types is self.source: - directory of files - file - url : http / rtsp - webcam If input source is a directory of files, then node will have specific methods to handle it. Otherwise opencv can deal with all non-directory sources. \"\"\" path = Path ( self . source ) if isinstance ( self . source , int ): self . _source_type = SourceType . WEBCAM elif str ( self . source ) . startswith (( \"http://\" , \"https://\" , \"rtsp://\" )): self . _source_type = SourceType . URL self . _file_name = path . name else : # either directory or file if not path . exists (): raise FileNotFoundError ( f \"Path ' { path } ' does not exist\" ) if path . is_dir (): self . _source_type = SourceType . DIRECTORY else : self . _source_type = SourceType . FILE self . _file_name = path . name Explanation In the run method of input.visual.py , we note that the output dict is called by outputs = self._get_next_frame() and self._file_name is therefore crucial to output the correct filename of the source. Since source is set as self.source , we use pathlib.Path to convert it to a pathlib.Path object and call the name property to get the filename. Show/Hide content for _get_next_frame() def _get_next_frame ( self ) -> Dict [ str , Any ]: \"\"\"Read next frame from current input file/source\"\"\" self . file_end = True outputs = { \"img\" : None , \"filename\" : self . _file_name if self . _file_name else self . filename , \"pipeline_end\" : True , \"saved_video_fps\" : self . _fps if self . _fps > 0 else self . saved_video_fps , } if self . videocap : success , img = self . videocap . read_frame () if success : self . file_end = False if self . do_resize : img = resize_image ( img , self . resize [ \"width\" ], self . resize [ \"height\" ] ) outputs [ \"img\" ] = img outputs [ \"pipeline_end\" ] = False else : self . logger . debug ( \"No video frames available for processing.\" ) return outputs custom_nodes.dabble.exercise_counter dabble/exercise_counter.yml Running peekingduck create-node command creates a default configuration for the custom_nodes.dabble.exercise_counter node: Default config for dabble/exercise_counter.yml 1 2 3 4 input : [ \"bboxes\" , \"bbox_labels\" ] # (1) output : [ \"obj_attrs\" , \"custom_key\" ] # (2) threshold : 0.5 # (3) Mandatory configs. The default configurations receive bounding boxes and their respective labels as input. Replace with other data types as required. List of built-in data types for PeekingDuck can be found in here . Output obj_attrs for visualization with draw.tag node and custom_key for use with other custom nodes. Replace as required. Optional configs depending on node. We will go through that later. We need to edit the file according to our own needs. Since we are chaining Yolo and MoveNet , this node should therefore take in img , bboxes , bbox_scores , keypoints , and keypoint_scores as inputs from the pipeline and outputs a dictionary with keys such as frame_count , num_push_ups , body_direction , elbow_angle , shoulder_keypoint , elbow_keypoint , and wrist_keypoint . We also want to define some optional configuration items for the dabble.exercise_counter node. For example, users may specify a custom exercise name in the exercise_name configuration item. For now, the only supported value is push_ups . Consequently, the newly updated dabble.exercise_counter.yml file should contain the following: dabble.exercise_counter.yml 1 2 3 4 5 6 7 8 9 10 11 input : [ \"img\" , \"bboxes\" , \"bbox_scores\" , \"keypoints\" , \"keypoint_scores\" ] # (1) output : [ \"frame_count\" , \"num_push_ups\" , \"expected_pose\" , \"elbow_angle\" , \"shoulder_keypoint\" , \"elbow_keypoint\" , \"wrist_keypoint\" , \"filename\" ] # (2) # Optional configs keypoint_threshold : 0.3 # (3) exercise_name : \"push_ups\" # (4) push_up_pose_params : { starting_elbow_angle : 155 , ending_elbow_angle : 90 , } # (5) The inputs propagated from previous nodes. img : Input image bboxes : Bounding boxes bbox_scores : Bounding box scores keypoints : Keypoints keypoint_scores : Keypoint scores The outputs of the current node. frame_count : Incremented every time a new frame is processed. num_push_ups : The number of push-ups detected in the current frame. body_direction : The direction of the body in the current frame. elbow_angle : The angle between the wrist, elbow and the shoulder in the current frame. shoulder_keypoint : The keypoint of the shoulder in the current frame. elbow_keypoint : The keypoint of the elbow in the current frame. wrist_keypoint : The keypoint of the wrist in the current frame. This is an optional configuration parameter that will be initialized if defined. Here I defined a keypoint_threshold parameter. This is an optional configuration parameter that will be initialized if defined. Here I defined a exercise_name parameter. This is an optional configuration parameter that will be initialized if defined. Here I defined a push_up_pose_params parameter. Let us walk through the mandatory inputs and optional configs of dabble.exercise_counter.yml in the next two sections. One should also refer back by clicking the \u2795 beside each line of code in the above file for annotations. Mandatory Default Configuration Items All nodes must specify input and output items in their default configuration file. All items specified in the input array must be computed and returned by previous nodes in the pipeline. For the dabble.exercise_counter node, we rely on previous nodes to provide inputs for our exercise detection algorithm, such as the detected keypoints. Warning Since both PoseNet/MoveNet and Yolo have the same output key bboxes , chaining PoseNet/Movenet after Yolo will cause the common output bboxes to be overwritten by the latter. Optional Configs Recall the optional configs defined in dabble.exercise_counter.yml : optional config 1 2 3 4 5 6 7 # Optional configs keypoint_threshold : 0.3 exercise_name : \"push_ups\" push_up_pose_params : { starting_elbow_angle : 155 , ending_elbow_angle : 90 , } These configuration items will set as class instance attributes of the custom node Node(AbstractNode) upon initialization. abstract node class 1 2 3 4 5 6 7 8 9 class Node ( AbstractNode ): \"\"\"This is a template class of how to write a node for PeekingDuck. Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) Info For example, when you pass an additional parameter exercise_name in exercise_counter.yml , this parameter will be instantiated in the dabble.exercise_counter Node as an attribute self.exercise_name . A quick check in the source code peekingduck.pipeline.nodes.node.py shows that it will set all the keys defined in the .yml file. # sets class attributes for key in self . config : setattr ( self , key , self . config [ key ]) Note As an aside, the other optional argument passed is keypoint_threshold which is used to determine the threshold for the keypoints. Technically it behaves the same as the keypoint_score_threshold in model.movenet 's Configs. dabble/exercise_counter.py The dabble.exercise_counter implements our push-up counter. Fig 1: Push-up Image by Fortune Vieyra via Unsplash - Copyright-free With the assumptions in the earlier section, our heuristic is: As shown in Figure 1, let the person's left shoulder, elbow and wrist be point (keypoints of \\((x,y)\\) coordinates) \\(A\\) , \\(B\\) and \\(C\\) respectively. Then the angle \\(\\angle{ABC}\\) formed by the line vector \\(\\vec{BA}\\) and \\(\\vec{BC}\\) is the elbow angle. Define the following: \\(\\angle{S}=155^{\\circ}\\) to be the threshold for starting elbow angle; \\(\\angle{E}=90^{\\circ}\\) to be the threshold fpr ending elbow angle 5 ; \\(N=0\\) as the number of push-ups; \\(H=\\text{False}\\) as whether the person has started performing push-ups, and; \\(P\\in\\{\\text{up}, \\text{down}\\}=\\text{down}\\) as the expected pose. \\({\\color{red} \\text{We say that the person is in an up pose if} \\angle{ABC} > S, \\text{and in a down pose if} \\angle{ABC} \\le E.}\\) Once the person assumes an \\(\\text{up}\\) pose for the first time (i.e. getting ready for push up), set \\(H=\\text{True}\\) . If \\(H=\\text{True}\\) , and the person assumes a \\(\\text{down}\\) pose, and \\(P=\\text{down}\\) , set \\(N=N+0.5\\) and \\(P=\\text{up}\\) . Otherwise, if \\(H=\\text{True}\\) and the person assumes an \\(\\text{up}\\) pose, and \\(P=\\text{up}\\) , set \\(N=N+0.5\\) and \\(P=\\text{down}\\) . We detect the person performing a push-up as a cycle. A push-up starts when the person has an \\(\\text{up}\\) pose. When the person moves to a \\(\\text{down}\\) pose, they have completed half the cycle, so we increment \\(N\\) by \\(0.5\\) . When they return to an \\(\\text{up}\\) pose, they have completed the cycle, so \\(N\\) is incremented by \\(0.5\\) again, giving a total incremnt of \\(1\\) per cycle. This code below implements the logic detailed previously. Note Note that there are three helper functions map_bbox_to_image_coords , map_keypoint_to_image_coords and draw_text to convert relative to absolute coordinates and to draw text on-screen. These are taken from PeekingDuck's Tutorial on counting hand waves . Show/Hide code for dabble/exercise_counter.py dabble/exercise_counter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @dataclass ( frozen = True ) class GlobalParams : \"\"\"Global parameters for the node. Attributes: FONT (int): Font for the text. WHITE (int): White color in BGR. YELLOW (int): Yellow color in BGR. KP_NAME_TO_INDEX (Dict[str, int]): PoseNet/MoveNet's skeletal keypoints name to index mapping. \"\"\" FONT : int = cv2 . FONT_HERSHEY_SIMPLEX WHITE : Tuple [ int ] = ( 255 , 255 , 255 ) YELLOW : Tuple [ int ] = ( 0 , 255 , 255 ) KP_NAME_TO_INDEX : Dict [ str , int ] = field ( default_factory = lambda : { \"nose\" : 0 , \"left_eye\" : 1 , \"right_eye\" : 2 , \"left_ear\" : 3 , \"right_ear\" : 4 , \"left_shoulder\" : 5 , \"right_shoulder\" : 6 , \"left_elbow\" : 7 , \"right_elbow\" : 8 , \"left_wrist\" : 9 , \"right_wrist\" : 10 , \"left_hip\" : 11 , \"right_hip\" : 12 , \"left_knee\" : 13 , \"right_knee\" : 14 , \"left_ankle\" : 15 , \"right_ankle\" : 16 , } ) @dataclass ( frozen = False ) class PushupPoseParams : \"\"\"Push up pose parameters. Attributes: starting_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for starting (up) pose. ending_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for ending (down) pose. \"\"\" starting_elbow_angle : float ending_elbow_angle : float @classmethod def from_dict ( cls : Type [ \"PushupPoseParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"PushupPoseParams\" ]: \"\"\"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Args: params_dict (Dict[str, Any]): Dictionary of parameters. Returns: (PushupPoseParams): Dataclass with the parameters initalized. \"\"\" return cls ( starting_elbow_angle = params_dict [ \"starting_elbow_angle\" ], ending_elbow_angle = params_dict [ \"ending_elbow_angle\" ], ) class Node ( AbstractNode ): \"\"\"Custom node to display keypoints and count number of hand waves Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. config.exercise_name (str): Name of the exercise. Default: \"pushups\". config.keypoint_threshold (float): Ignore keypoints below this threshold. Default: 0.3. config.push_up_pose_params (:obj:`Dict[str, Any]`): Parameters for the push up pose. Default: {starting_elbow_angle: 155, ending_elbow_angle: 90}. Attributes: self.frame_count (int): Track the number of frames processed. self.expected_pose (str): The expected pose. Default: \"down\". self.num_push_ups (float): Cumulative number of push ups. Default: 0. self.have_started_push_ups (bool): Whether or not the push ups have started. Default: False. self.elbow_angle (float): Angle of the elbow. Default: None. self.global_params_dataclass (GlobalParams): Global parameters for the node. self.push_up_pose_params_dataclass (PushupPoseParams): Push up pose parameters. self.interested_keypoints (List[str]): List of keypoints to track. Default: [\"left_elbow\", \"left_shoulder\", \"left_wrist\"]. self.left_elbow (float): Keypoints of the left elbow. Default: None. self.left_shoulder (float): Keypoints of the left shoulder. Default: None. self.left_wrist (float): Keypoints of the left wrist. Default: None. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) self . logger = logging . getLogger ( __name__ ) self . exercise_name : str self . keypoint_threshold : float # ignore keypoints below this threshold self . push_up_pose_params : Dict [ str , Any ] self . logger . info ( f \"Initialize Exercise Type: { self . exercise_name } !\" ) self . frame_count = 0 self . expected_pose = \"down\" self . num_push_ups = 0 self . have_started_push_ups = False self . elbow_angle = None self . global_params_dataclass = GlobalParams () self . push_up_pose_params_dataclass = PushupPoseParams . from_dict ( self . push_up_pose_params ) self . interested_keypoints = [ \"left_elbow\" , \"left_shoulder\" , \"left_wrist\" , ] # each element in the self.interested_keypoints list will now become an attribute initialized to None self . reset_keypoints_to_none () def reset_keypoints_to_none ( self ) -> None : \"\"\"Reset all keypoints attributes to None after each frame.\"\"\" for interested_keypoint in self . interested_keypoints : setattr ( self , interested_keypoint , None ) def inc_num_push_ups ( self ) -> float : \"\"\"Increments the number of push ups by 0.5 for every directional change. Returns: self.num_push_ups (float): Cumulative number of push ups. \"\"\" self . num_push_ups += 0.5 return self . num_push_ups def is_up_pose ( self , elbow_angle : float ) -> bool : \"\"\"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a up pose, False otherwise. \"\"\" return ( elbow_angle > self . push_up_pose_params_dataclass . starting_elbow_angle ) def is_down_pose ( self , elbow_angle : float ) -> None : \"\"\"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a down pose, False otherwise. \"\"\" return ( elbow_angle <= self . push_up_pose_params_dataclass . ending_elbow_angle ) @staticmethod def is_bbox_or_keypoints_empty ( bboxes : np . ndarray , keypoints : np . ndarray , keypoint_scores : np . ndarray , ) -> bool : \"\"\"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Args: bboxes (np.ndarray): The bounding boxes. keypoints (np.ndarray): The keypoints. keypoint_scores (np.ndarray): The keypoint scores. Returns: bool: True if the bounding box or keypoints are empty, False otherwise. \"\"\" return ( len ( bboxes ) == 0 or len ( keypoints ) == 0 or len ( keypoint_scores ) == 0 ) # pylint: disable=too-many-locals def count_push_ups ( self , img : np . ndarray , img_size : Tuple [ int , int ], the_keypoints : np . ndarray , the_keypoint_scores : np . ndarray , ) -> None : \"\"\"Counts the number of push ups. Args: img (np.ndarray): The image in each frame. img_size (Tuple[int, int]): The width and height of the image in each frame. the_keypoints (np.ndarray): The keypoints predicted in each frame. the_keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. \"\"\" interested_keypoints_names_to_index = { self . global_params_dataclass . KP_NAME_TO_INDEX [ interested_keypoint ]: interested_keypoint for interested_keypoint in self . interested_keypoints } self . reset_keypoints_to_none () for keypoint_idx , ( keypoints , keypoint_score ) in enumerate ( zip ( the_keypoints , the_keypoint_scores ) ): if keypoint_score >= self . keypoint_threshold : x , y = map_keypoint_to_image_coords ( keypoints . tolist (), img_size ) x_y_str = f \"( { x } , { y } )\" if keypoint_idx in interested_keypoints_names_to_index : keypoint_name = interested_keypoints_names_to_index [ keypoint_idx ] setattr ( self , keypoint_name , ( x , y )) the_color = self . global_params_dataclass . YELLOW else : the_color = self . global_params_dataclass . WHITE draw_text ( img , x_y_str , ( x , y ), color = the_color , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.4 , thickness = 2 , ) # all keypoints must be non-none if self . left_elbow and self . left_shoulder and self . left_wrist : left_elbow_angle = self . calculate_angle_using_dot_prod ( self . left_shoulder , self . left_elbow , self . left_wrist ) self . elbow_angle = left_elbow_angle # Check to ensure right form before starting the program if self . is_up_pose ( left_elbow_angle ): self . have_started_push_ups = True # Check for full range of motion for the pushup if self . have_started_push_ups : # the two if-statements are mutually exclusive: won't happen at the same time. if ( self . is_down_pose ( left_elbow_angle ) and self . expected_pose == \"down\" ): self . inc_num_push_ups () self . expected_pose = \"up\" if ( self . is_up_pose ( left_elbow_angle ) and self . expected_pose == \"up\" ): self . inc_num_push_ups () self . expected_pose = \"down\" pushup_str = f \"#push_ups = { self . num_push_ups } \" draw_text ( img , pushup_str , ( 20 , 30 ), color = self . global_params_dataclass . YELLOW , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1 , thickness = 3 , ) # pylint: disable=trailing-whitespace @staticmethod def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ) -> float : r \"\"\"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: $$ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} $$ Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Returns: angle (float): Angle between vectors BA and BC in radians or degrees. Shape: - Input: - a (np.ndarray): (2, ) - b (np.ndarray): (2, ) - c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) # arccos range is [0, pi] angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node draws keypoints and counts the number of push ups. Args: inputs (dict): Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. Note: To check the shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to [PeekingDuck API Documentation](https://peekingduck.readthedocs.io/en/stable/nodes/model.movenet.html#module-model.movenet). Returns: outputs (dict): Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. \"\"\" # get required inputs from pipeline img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] filename = inputs [ \"filename\" ] # image width, height img_size = ( img . shape [ 1 ], img . shape [ 0 ]) # frame count should not be in the if-clause self . frame_count += 1 if not self . is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ): # assume each frame has only one person; # note this bbox is from the posenet/movenet and not from yolo. the_bbox = bboxes [ 0 ] # bbox_scores are from yolo and not posenet/movenet. the_bbox_score = bbox_scores [ 0 ] if len ( bbox_scores ) > 0 else 0 x1 , _y1 , _x2 , y2 = map_bbox_to_image_coords ( the_bbox , img_size ) score_str = f \"BBox { the_bbox_score : 0.2f } \" # get bounding box confidence score and draw it at the left-bottom # (x1, y2) corner of the bounding box (offset by 30 pixels) draw_text ( img , score_str , ( x1 , y2 - 30 ), color = self . global_params_dataclass . WHITE , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1.0 , thickness = 3 , ) # assume each frame has only one person; the_keypoints = keypoints [ 0 ] the_keypoint_scores = keypoint_scores [ 0 ] # count the number of push ups self . count_push_ups ( img , img_size , the_keypoints , the_keypoint_scores ) # careful not to indent this return statement; # if the if-clause is false, then no dict will be returned and will crash the pipeline return { \"filename\" : filename , \"expected_pose\" : self . expected_pose , \"num_push_ups\" : self . num_push_ups , \"frame_count\" : self . frame_count , \"elbow_angle\" : self . elbow_angle , \"elbow_keypoint\" : self . left_elbow , \"shoulder_keypoint\" : self . left_shoulder , \"wrist_keypoint\" : self . left_wrist , } output/csv_writer.yml This is a default node and we will make use of the output.csv_writer node to write the results from exercise_counter to a CSV file. A quick check at the default settings from peekingduck.configs.output.csv_writer.yml yields: default output.csv_writer.yml 1 2 3 4 5 6 input : [ \"all\" ] output : [ \"none\" ] stats_to_track : [ \"keypoints\" , \"bboxes\" , \"bbox_labels\" ] file_path : \"PeekingDuck/data/stats.csv\" logging_interval : 1 # in terms of seconds between each log We are fine with input and output as they are but need to modify the configurations from [ lines 4 - 6 ] . Since the configurations' key names are not changed, we can directly overwrite them in pipeline_config.yml as follows: pipeline_config.yml 1 2 3 4 5 6 7 8 - output.csv_writer : stats_to_track : [ \"keypoints\" , \"bboxes\" , \"bbox_labels\" , \"num_push_ups\" , \"frame_count\" , \"body_direction\" , \"elbow_angle\" , \"shoulder_keypoint\" , \"elbow_keypoint\" , \"wrist_keypoint\" , \"filename\" ] # (1) file_path : \"./stores/artifacts/push_ups_output_movenet.csv\" # (2) logging_interval : 0 # (3) The main aim is to keep track of keypoints information per frame. The file_path is set to ./store/artifacts/push_ups_output_movenet.csv . This is where the CSV file will be written to. The logging_interval is set to 0. This means that the CSV file will be written to every frame. The code block from peekingduck.pipeline.nodes.output.utils.csvlogger shows how the logging_interval logic is implemented. if ( curr_time - self . last_write ) . seconds >= self . logging_interval : self . writer . writerow ( content ) self . last_write = curr_time Info A small recap, this node is used to write the results to a CSV file and is chained after the dabble.exercise_counter node. We pass the output dict of the dabble.exercise_counter as inputs to ( stats_to_track ). Interpretation of CSV outputs The snippet below shows the CSV file contents. Fig 2: Push-up Counter CSV Keypoints of elbow, shoulder, wrist as well as elbow angles are recorded every frame, if some of them are not detected by the model, it will be None and recorded as an empty string in the CSV file. If any of elbow, shoulder and wrist keypoints are None , then the corresponding elbow angle will be the same as the previous frame. References This is my first encounter with Pose Estimation. Here are some references that I draw inspiration from. Next-Generation Pose Detection with MoveNet and TensorFlow.js Multi-Person Pose Estimation with Mediapipe How I created the Workout Movement Counting App using Deep Learning and Optical Flow Algorithm Human Pose Classification with MoveNet and TensorFlow Lite MoveNet: Ultra fast and accurate pose detection model Deep learning approaches for workout repetition counting and validation Push-up counter using Mediapipe python Pose Classification From MediaPipe RepCounter using PoseNet Exercise Reps Counter || Pose Estimation Deep Learning Exercise Repetitions Counter Real-time Human Pose Estimation in the Browser with TensorFlow.js Fitness Camera \u2013 Turn Your Phone's Camera Into a Fitness Tracker Keypoint IDs \u21a9 Coordinate Systems \u21a9 You can read more about pipeline config in PeekingDuck's Documentation . \u21a9 There are various ways to create custom nodes. See more from the PeekingDuck documentation . \u21a9 The values for \\(S\\) and \\(E\\) are configurable, but \\(155^{\\circ}\\) and \\(90^{\\circ}\\) are good defaults. \u21a9","title":"Exercise Counter"},{"location":"exercise_counter/#push-up-counter","text":"The aim of this project is to build an exercise counter that can be used to detect push-ups, and more generic exercises in future iterations. Some content below are referenced from PeekingDuck's Tutorial . The main model is MoveNet , which outputs seventeen keypoints for the person corresponding to the different body parts as documented here 1 . Each keypoint is a pair of \\((x, y)\\) coordinates, where \\(x\\) and \\(y\\) are real numbers ranging from \\(0.0\\) to \\(1.0\\) (using relative coordinates 2 ). Note This is a Minimum Viable Product (MVP) and uses simple logic to detect push-ups. Consequently, there are a few somewhat rigid assumptions that will be made in the code. We will detail the logic in the following sections.","title":"Push-Up Counter"},{"location":"exercise_counter/#assumptions","text":"This project makes a few assumptions about the input data for simplicity of implementation: The user's left body parts are visible through the webcam/video and in particular, the left elbow, wrist and shoulder are crucial for our push-up counter. We can improve on this rigid requirement in the future by checking both the left and right body parts, and take the side with higher keypoints confidence . There should be only \\(1\\) person in the video. As we are using MoveNet's singlepose_thunder model, we need to impose the restriction that there should be only \\(1\\) person in the video to avoid performance issues. We can improve on this rigid requirement in the future by incorporating multi-person logic. The multipose_lightning can detect up to \\(6\\) people in the video. If user leaves the video and come back, we assume that the user is the same person and continue counting.","title":"Assumptions"},{"location":"exercise_counter/#custom-node-general-workflow","text":"","title":"Custom Node General Workflow"},{"location":"exercise_counter/#step-1-initialize-peekingduck-template","text":"After setting up from the Workflows section, we can start using the PeekingDuck interface. We initialize a new PeekingDuck project using the following commands: Terminal Session Initializing PeekingDuck 1 2 3 $ mkdir custom_hn_exercise_counter $ cd custom_hn_exercise_counter $ peekingduck init [ Line 1 ] : Create a new directory named custom_hn_exercise_counter for the project. [ Line 2 ] : Change to the newly created directory. [ Line 3 ] : Initialize the PeekingDuck project in the current directory with default file/folder below. Upon initialization of the project, PeekingDuck creates the following files in your new project directory: pipeline_config.yml - Contains the pipeline 3 configuration and; src/ - Folder for custom nodes. The custom_hn_exercise_counter directory currently looks like this: Directory Tree of custom_hn_exercise_counter custom_hn_exercise_counter/ \u251c\u2500\u2500 pipeline_config.yml \u2514\u2500\u2500 src/ \u2514\u2500\u2500 custom_nodes/ \u2514\u2500\u2500configs/","title":"Step 1. Initialize PeekingDuck Template"},{"location":"exercise_counter/#step-2-use-pipeline-recipe-to-create-custom-nodes","text":"PeekingDuck provides several node types out of the box, for example a MoveNet node to detect human poses within an image. To implement additional functionality not provided by built-in nodes, we can create custom nodes with our own logic written in Python. For our project, we need to create two custom nodes. First, we will create a custom_nodes.input.visual node. Although PeekingDuck provides a visual input node, a small issue prevents filenames from URL visual sources from registering properly. Registering the filename properly is important, because we use it in later stages of our pipeline, including writing output data to CSV. To fix this issue, we implement this custom node. Additionally, we implement a custom_nodes.dabble.exercise_counter node. This node will take the keypoints from the MoveNet model and count the number of push-ups performed by the person in the input video. To create these custom nodes, we first edit our pipeline_config.yml : Show/Hide content for pipeline_config.yml pipeline_config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 nodes : - custom_nodes.input.visual : source : https://storage.googleapis.com/reighns/peekingduck/videos/push_ups.mp4 - model.yolo : model_type : \"v4tiny\" iou_threshold : 0.1 score_threshold : 0.1 detect_ids : [ \"person\" ] # [0] num_classes : 1 #- custom_nodes.dabble.debug_yolo - model.movenet : model_type : \"singlepose_thunder\" resolution : { height : 256 , width : 256 } bbox_score_threshold : 0.05 keypoint_score_threshold : 0.05 - dabble.fps - custom_nodes.dabble.exercise_counter : keypoint_threshold : 0.3 exercise_name : \"push_ups\" push_up_pose_params : { starting_elbow_angle : 155 , ending_elbow_angle : 90 , } - draw.poses - draw.legend : show : [ \"fps\" ] - output.csv_writer : stats_to_track : [ \"keypoints\" , \"bboxes\" , \"bbox_labels\" , \"num_push_ups\" , \"frame_count\" , \"expected_pose\" , \"elbow_angle\" , \"shoulder_keypoint\" , \"elbow_keypoint\" , \"wrist_keypoint\" , \"filename\" ] file_path : \"./stores/artifacts/push_ups_output_movenet.csv\" logging_interval : 0 - output.screen Adding our custom nodes (highlighted) is as simple as adding entries for them to the pipeline_config.yml file. Additionally, we add configuration for some nodes. For example, I specify that I want to use v4tiny model for the model.yolo node with a \\(0.1\\) threshold for both iou and bounding box confidence score. These configurations will then be passed to the config parameter of the model.yolo node. We then create the custom nodes using the Pipeline Recipe method 4 with the following command: Terminal Session Creating Custom Nodes $ peekingduck create-node --config_path pipeline_config.yml This will create all the nodes listed in the pipeline_config.yml file. If one decides to add more custom nodes, we can simply add them to the pipeline_config.yml file and run the command again. The updated custom_hn_exercise_counter directory currently looks like this: Directory Tree of custom_hn_exercise_counter custom_hn_exercise_counter/ \u251c\u2500\u2500 pipeline_config.yml \u2514\u2500\u2500 src/ \u2514\u2500\u2500 custom_nodes/ \u251c\u2500\u2500 configs/ | \u251c\u2500\u2500 dabble/ | | \u2514\u2500\u2500 exercise_counter.yml | \u2514\u2500\u2500 input/ | \u2514\u2500\u2500 visual.yml \u251c\u2500\u2500 dabble/ | \u2514\u2500\u2500 exercise_counter.py \u2514\u2500\u2500 input/ \u2514\u2500\u2500 visual.py custom_hn_exercise_counter now contains five files that we need to modify in order to implement our custom push-up counter. pipeline_config.yml src/custom_nodes/dabble/exercise_counter.py src/custom_nodes/configs/dabble/exercise_counter.yml src/custom_nodes/input/visual.py src/custom_nodes/configs/input/visual.py We will go into details in the next section . We will also create an additional folder stores in the same level as src . For now, we will add a folder artifacts in the stores folder, this is where we will store the output files and model artifacts. Directory Tree of custom_hn_exercise_counter custom_hn_exercise_counter/ \u251c\u2500\u2500 src/ \u251c\u2500\u2500 stores/ \u2502 \u2514\u2500\u2500 artifacts/ \u2514\u2500\u2500 pipeline_config.yml","title":"Step 2. Use Pipeline Recipe to Create Custom Nodes"},{"location":"exercise_counter/#step-3-deep-dive-into-the-custom-nodes","text":"After Step 2 , three folders config , dabble and input will be populated in src . The config folder holds the configurations for the custom nodes while the dabble and input folders holds the code for the custom nodes. Info Something worth noting is that other default nodes that are in PeekingDuck will not be included in the config folder. For example, the model.yolo node is not included in the config folder because it is a default node . This is because the configurations for the default nodes are already included in the pipeline_config.yml file and will be instantiated when peekingduck run is called.","title":"Step 3. Deep Dive into the Custom Nodes"},{"location":"exercise_counter/#custom_nodesinputvisual","text":"","title":"custom_nodes.input.visual"},{"location":"exercise_counter/#inputvisualyml","text":"When defining a custom node, we must provide a default configuration. We can use the default configuration from the built-in input.visual node: Show/Hide content for input.visual.yml input.visual.yml input : [ \"none\" ] output : [ \"img\" , \"filename\" , \"pipeline_end\" , \"saved_video_fps\" ] filename : video.mp4 frames_log_freq : 100 mirror_image : False resize : { do_resizing : False , width : 1280 , height : 720 } saved_video_fps : 10 source : https://storage.googleapis.com/peekingduck/videos/wave.mp4 threading : False buffering : False","title":"input/visual.yml"},{"location":"exercise_counter/#inputvisualpy","text":"In PeekingDuck version 1.2.0 , if you define source to be a URL , the filename is not overwritten by the source filename and maintains the default video.mp4 . There is a pull request to fix a similar issue where the filename was not set if the source is a single image. In our custom_nodes.input.visual node, we apply a similar fix for the URL case. This change will help us save the filename parameter in the output CSV file later. Standing on the shoulder of giants, I quickly modified a few lines to suit my purpose as I want to save the filename parameter in my output csv file later. Therefore, the code inside this file is mostly the same except for the highlighted lines below. To implement this custom node, copy the peekingduck/pipeline/nodes/input/visual.py file from the PeekingDuck dev branch to custom_hn_exercise_counter/src/custom_nodes/input/visual.py and update the highlighted lines: input.visual.py: _determine_source_type() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def _determine_source_type ( self ) -> None : \"\"\" Determine which one of the following types is self.source: - directory of files - file - url : http / rtsp - webcam If input source is a directory of files, then node will have specific methods to handle it. Otherwise opencv can deal with all non-directory sources. \"\"\" path = Path ( self . source ) if isinstance ( self . source , int ): self . _source_type = SourceType . WEBCAM elif str ( self . source ) . startswith (( \"http://\" , \"https://\" , \"rtsp://\" )): self . _source_type = SourceType . URL self . _file_name = path . name else : # either directory or file if not path . exists (): raise FileNotFoundError ( f \"Path ' { path } ' does not exist\" ) if path . is_dir (): self . _source_type = SourceType . DIRECTORY else : self . _source_type = SourceType . FILE self . _file_name = path . name Explanation In the run method of input.visual.py , we note that the output dict is called by outputs = self._get_next_frame() and self._file_name is therefore crucial to output the correct filename of the source. Since source is set as self.source , we use pathlib.Path to convert it to a pathlib.Path object and call the name property to get the filename. Show/Hide content for _get_next_frame() def _get_next_frame ( self ) -> Dict [ str , Any ]: \"\"\"Read next frame from current input file/source\"\"\" self . file_end = True outputs = { \"img\" : None , \"filename\" : self . _file_name if self . _file_name else self . filename , \"pipeline_end\" : True , \"saved_video_fps\" : self . _fps if self . _fps > 0 else self . saved_video_fps , } if self . videocap : success , img = self . videocap . read_frame () if success : self . file_end = False if self . do_resize : img = resize_image ( img , self . resize [ \"width\" ], self . resize [ \"height\" ] ) outputs [ \"img\" ] = img outputs [ \"pipeline_end\" ] = False else : self . logger . debug ( \"No video frames available for processing.\" ) return outputs","title":"input/visual.py"},{"location":"exercise_counter/#custom_nodesdabbleexercise_counter","text":"","title":"custom_nodes.dabble.exercise_counter"},{"location":"exercise_counter/#dabbleexercise_counteryml","text":"Running peekingduck create-node command creates a default configuration for the custom_nodes.dabble.exercise_counter node: Default config for dabble/exercise_counter.yml 1 2 3 4 input : [ \"bboxes\" , \"bbox_labels\" ] # (1) output : [ \"obj_attrs\" , \"custom_key\" ] # (2) threshold : 0.5 # (3) Mandatory configs. The default configurations receive bounding boxes and their respective labels as input. Replace with other data types as required. List of built-in data types for PeekingDuck can be found in here . Output obj_attrs for visualization with draw.tag node and custom_key for use with other custom nodes. Replace as required. Optional configs depending on node. We will go through that later. We need to edit the file according to our own needs. Since we are chaining Yolo and MoveNet , this node should therefore take in img , bboxes , bbox_scores , keypoints , and keypoint_scores as inputs from the pipeline and outputs a dictionary with keys such as frame_count , num_push_ups , body_direction , elbow_angle , shoulder_keypoint , elbow_keypoint , and wrist_keypoint . We also want to define some optional configuration items for the dabble.exercise_counter node. For example, users may specify a custom exercise name in the exercise_name configuration item. For now, the only supported value is push_ups . Consequently, the newly updated dabble.exercise_counter.yml file should contain the following: dabble.exercise_counter.yml 1 2 3 4 5 6 7 8 9 10 11 input : [ \"img\" , \"bboxes\" , \"bbox_scores\" , \"keypoints\" , \"keypoint_scores\" ] # (1) output : [ \"frame_count\" , \"num_push_ups\" , \"expected_pose\" , \"elbow_angle\" , \"shoulder_keypoint\" , \"elbow_keypoint\" , \"wrist_keypoint\" , \"filename\" ] # (2) # Optional configs keypoint_threshold : 0.3 # (3) exercise_name : \"push_ups\" # (4) push_up_pose_params : { starting_elbow_angle : 155 , ending_elbow_angle : 90 , } # (5) The inputs propagated from previous nodes. img : Input image bboxes : Bounding boxes bbox_scores : Bounding box scores keypoints : Keypoints keypoint_scores : Keypoint scores The outputs of the current node. frame_count : Incremented every time a new frame is processed. num_push_ups : The number of push-ups detected in the current frame. body_direction : The direction of the body in the current frame. elbow_angle : The angle between the wrist, elbow and the shoulder in the current frame. shoulder_keypoint : The keypoint of the shoulder in the current frame. elbow_keypoint : The keypoint of the elbow in the current frame. wrist_keypoint : The keypoint of the wrist in the current frame. This is an optional configuration parameter that will be initialized if defined. Here I defined a keypoint_threshold parameter. This is an optional configuration parameter that will be initialized if defined. Here I defined a exercise_name parameter. This is an optional configuration parameter that will be initialized if defined. Here I defined a push_up_pose_params parameter. Let us walk through the mandatory inputs and optional configs of dabble.exercise_counter.yml in the next two sections. One should also refer back by clicking the \u2795 beside each line of code in the above file for annotations.","title":"dabble/exercise_counter.yml"},{"location":"exercise_counter/#mandatory-default-configuration-items","text":"All nodes must specify input and output items in their default configuration file. All items specified in the input array must be computed and returned by previous nodes in the pipeline. For the dabble.exercise_counter node, we rely on previous nodes to provide inputs for our exercise detection algorithm, such as the detected keypoints. Warning Since both PoseNet/MoveNet and Yolo have the same output key bboxes , chaining PoseNet/Movenet after Yolo will cause the common output bboxes to be overwritten by the latter.","title":"Mandatory Default Configuration Items"},{"location":"exercise_counter/#optional-configs","text":"Recall the optional configs defined in dabble.exercise_counter.yml : optional config 1 2 3 4 5 6 7 # Optional configs keypoint_threshold : 0.3 exercise_name : \"push_ups\" push_up_pose_params : { starting_elbow_angle : 155 , ending_elbow_angle : 90 , } These configuration items will set as class instance attributes of the custom node Node(AbstractNode) upon initialization. abstract node class 1 2 3 4 5 6 7 8 9 class Node ( AbstractNode ): \"\"\"This is a template class of how to write a node for PeekingDuck. Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) Info For example, when you pass an additional parameter exercise_name in exercise_counter.yml , this parameter will be instantiated in the dabble.exercise_counter Node as an attribute self.exercise_name . A quick check in the source code peekingduck.pipeline.nodes.node.py shows that it will set all the keys defined in the .yml file. # sets class attributes for key in self . config : setattr ( self , key , self . config [ key ]) Note As an aside, the other optional argument passed is keypoint_threshold which is used to determine the threshold for the keypoints. Technically it behaves the same as the keypoint_score_threshold in model.movenet 's Configs.","title":"Optional Configs"},{"location":"exercise_counter/#dabbleexercise_counterpy","text":"The dabble.exercise_counter implements our push-up counter. Fig 1: Push-up Image by Fortune Vieyra via Unsplash - Copyright-free With the assumptions in the earlier section, our heuristic is: As shown in Figure 1, let the person's left shoulder, elbow and wrist be point (keypoints of \\((x,y)\\) coordinates) \\(A\\) , \\(B\\) and \\(C\\) respectively. Then the angle \\(\\angle{ABC}\\) formed by the line vector \\(\\vec{BA}\\) and \\(\\vec{BC}\\) is the elbow angle. Define the following: \\(\\angle{S}=155^{\\circ}\\) to be the threshold for starting elbow angle; \\(\\angle{E}=90^{\\circ}\\) to be the threshold fpr ending elbow angle 5 ; \\(N=0\\) as the number of push-ups; \\(H=\\text{False}\\) as whether the person has started performing push-ups, and; \\(P\\in\\{\\text{up}, \\text{down}\\}=\\text{down}\\) as the expected pose. \\({\\color{red} \\text{We say that the person is in an up pose if} \\angle{ABC} > S, \\text{and in a down pose if} \\angle{ABC} \\le E.}\\) Once the person assumes an \\(\\text{up}\\) pose for the first time (i.e. getting ready for push up), set \\(H=\\text{True}\\) . If \\(H=\\text{True}\\) , and the person assumes a \\(\\text{down}\\) pose, and \\(P=\\text{down}\\) , set \\(N=N+0.5\\) and \\(P=\\text{up}\\) . Otherwise, if \\(H=\\text{True}\\) and the person assumes an \\(\\text{up}\\) pose, and \\(P=\\text{up}\\) , set \\(N=N+0.5\\) and \\(P=\\text{down}\\) . We detect the person performing a push-up as a cycle. A push-up starts when the person has an \\(\\text{up}\\) pose. When the person moves to a \\(\\text{down}\\) pose, they have completed half the cycle, so we increment \\(N\\) by \\(0.5\\) . When they return to an \\(\\text{up}\\) pose, they have completed the cycle, so \\(N\\) is incremented by \\(0.5\\) again, giving a total incremnt of \\(1\\) per cycle. This code below implements the logic detailed previously. Note Note that there are three helper functions map_bbox_to_image_coords , map_keypoint_to_image_coords and draw_text to convert relative to absolute coordinates and to draw text on-screen. These are taken from PeekingDuck's Tutorial on counting hand waves . Show/Hide code for dabble/exercise_counter.py dabble/exercise_counter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 @dataclass ( frozen = True ) class GlobalParams : \"\"\"Global parameters for the node. Attributes: FONT (int): Font for the text. WHITE (int): White color in BGR. YELLOW (int): Yellow color in BGR. KP_NAME_TO_INDEX (Dict[str, int]): PoseNet/MoveNet's skeletal keypoints name to index mapping. \"\"\" FONT : int = cv2 . FONT_HERSHEY_SIMPLEX WHITE : Tuple [ int ] = ( 255 , 255 , 255 ) YELLOW : Tuple [ int ] = ( 0 , 255 , 255 ) KP_NAME_TO_INDEX : Dict [ str , int ] = field ( default_factory = lambda : { \"nose\" : 0 , \"left_eye\" : 1 , \"right_eye\" : 2 , \"left_ear\" : 3 , \"right_ear\" : 4 , \"left_shoulder\" : 5 , \"right_shoulder\" : 6 , \"left_elbow\" : 7 , \"right_elbow\" : 8 , \"left_wrist\" : 9 , \"right_wrist\" : 10 , \"left_hip\" : 11 , \"right_hip\" : 12 , \"left_knee\" : 13 , \"right_knee\" : 14 , \"left_ankle\" : 15 , \"right_ankle\" : 16 , } ) @dataclass ( frozen = False ) class PushupPoseParams : \"\"\"Push up pose parameters. Attributes: starting_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for starting (up) pose. ending_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for ending (down) pose. \"\"\" starting_elbow_angle : float ending_elbow_angle : float @classmethod def from_dict ( cls : Type [ \"PushupPoseParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"PushupPoseParams\" ]: \"\"\"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Args: params_dict (Dict[str, Any]): Dictionary of parameters. Returns: (PushupPoseParams): Dataclass with the parameters initalized. \"\"\" return cls ( starting_elbow_angle = params_dict [ \"starting_elbow_angle\" ], ending_elbow_angle = params_dict [ \"ending_elbow_angle\" ], ) class Node ( AbstractNode ): \"\"\"Custom node to display keypoints and count number of hand waves Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. config.exercise_name (str): Name of the exercise. Default: \"pushups\". config.keypoint_threshold (float): Ignore keypoints below this threshold. Default: 0.3. config.push_up_pose_params (:obj:`Dict[str, Any]`): Parameters for the push up pose. Default: {starting_elbow_angle: 155, ending_elbow_angle: 90}. Attributes: self.frame_count (int): Track the number of frames processed. self.expected_pose (str): The expected pose. Default: \"down\". self.num_push_ups (float): Cumulative number of push ups. Default: 0. self.have_started_push_ups (bool): Whether or not the push ups have started. Default: False. self.elbow_angle (float): Angle of the elbow. Default: None. self.global_params_dataclass (GlobalParams): Global parameters for the node. self.push_up_pose_params_dataclass (PushupPoseParams): Push up pose parameters. self.interested_keypoints (List[str]): List of keypoints to track. Default: [\"left_elbow\", \"left_shoulder\", \"left_wrist\"]. self.left_elbow (float): Keypoints of the left elbow. Default: None. self.left_shoulder (float): Keypoints of the left shoulder. Default: None. self.left_wrist (float): Keypoints of the left wrist. Default: None. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) self . logger = logging . getLogger ( __name__ ) self . exercise_name : str self . keypoint_threshold : float # ignore keypoints below this threshold self . push_up_pose_params : Dict [ str , Any ] self . logger . info ( f \"Initialize Exercise Type: { self . exercise_name } !\" ) self . frame_count = 0 self . expected_pose = \"down\" self . num_push_ups = 0 self . have_started_push_ups = False self . elbow_angle = None self . global_params_dataclass = GlobalParams () self . push_up_pose_params_dataclass = PushupPoseParams . from_dict ( self . push_up_pose_params ) self . interested_keypoints = [ \"left_elbow\" , \"left_shoulder\" , \"left_wrist\" , ] # each element in the self.interested_keypoints list will now become an attribute initialized to None self . reset_keypoints_to_none () def reset_keypoints_to_none ( self ) -> None : \"\"\"Reset all keypoints attributes to None after each frame.\"\"\" for interested_keypoint in self . interested_keypoints : setattr ( self , interested_keypoint , None ) def inc_num_push_ups ( self ) -> float : \"\"\"Increments the number of push ups by 0.5 for every directional change. Returns: self.num_push_ups (float): Cumulative number of push ups. \"\"\" self . num_push_ups += 0.5 return self . num_push_ups def is_up_pose ( self , elbow_angle : float ) -> bool : \"\"\"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a up pose, False otherwise. \"\"\" return ( elbow_angle > self . push_up_pose_params_dataclass . starting_elbow_angle ) def is_down_pose ( self , elbow_angle : float ) -> None : \"\"\"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a down pose, False otherwise. \"\"\" return ( elbow_angle <= self . push_up_pose_params_dataclass . ending_elbow_angle ) @staticmethod def is_bbox_or_keypoints_empty ( bboxes : np . ndarray , keypoints : np . ndarray , keypoint_scores : np . ndarray , ) -> bool : \"\"\"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Args: bboxes (np.ndarray): The bounding boxes. keypoints (np.ndarray): The keypoints. keypoint_scores (np.ndarray): The keypoint scores. Returns: bool: True if the bounding box or keypoints are empty, False otherwise. \"\"\" return ( len ( bboxes ) == 0 or len ( keypoints ) == 0 or len ( keypoint_scores ) == 0 ) # pylint: disable=too-many-locals def count_push_ups ( self , img : np . ndarray , img_size : Tuple [ int , int ], the_keypoints : np . ndarray , the_keypoint_scores : np . ndarray , ) -> None : \"\"\"Counts the number of push ups. Args: img (np.ndarray): The image in each frame. img_size (Tuple[int, int]): The width and height of the image in each frame. the_keypoints (np.ndarray): The keypoints predicted in each frame. the_keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. \"\"\" interested_keypoints_names_to_index = { self . global_params_dataclass . KP_NAME_TO_INDEX [ interested_keypoint ]: interested_keypoint for interested_keypoint in self . interested_keypoints } self . reset_keypoints_to_none () for keypoint_idx , ( keypoints , keypoint_score ) in enumerate ( zip ( the_keypoints , the_keypoint_scores ) ): if keypoint_score >= self . keypoint_threshold : x , y = map_keypoint_to_image_coords ( keypoints . tolist (), img_size ) x_y_str = f \"( { x } , { y } )\" if keypoint_idx in interested_keypoints_names_to_index : keypoint_name = interested_keypoints_names_to_index [ keypoint_idx ] setattr ( self , keypoint_name , ( x , y )) the_color = self . global_params_dataclass . YELLOW else : the_color = self . global_params_dataclass . WHITE draw_text ( img , x_y_str , ( x , y ), color = the_color , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.4 , thickness = 2 , ) # all keypoints must be non-none if self . left_elbow and self . left_shoulder and self . left_wrist : left_elbow_angle = self . calculate_angle_using_dot_prod ( self . left_shoulder , self . left_elbow , self . left_wrist ) self . elbow_angle = left_elbow_angle # Check to ensure right form before starting the program if self . is_up_pose ( left_elbow_angle ): self . have_started_push_ups = True # Check for full range of motion for the pushup if self . have_started_push_ups : # the two if-statements are mutually exclusive: won't happen at the same time. if ( self . is_down_pose ( left_elbow_angle ) and self . expected_pose == \"down\" ): self . inc_num_push_ups () self . expected_pose = \"up\" if ( self . is_up_pose ( left_elbow_angle ) and self . expected_pose == \"up\" ): self . inc_num_push_ups () self . expected_pose = \"down\" pushup_str = f \"#push_ups = { self . num_push_ups } \" draw_text ( img , pushup_str , ( 20 , 30 ), color = self . global_params_dataclass . YELLOW , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1 , thickness = 3 , ) # pylint: disable=trailing-whitespace @staticmethod def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ) -> float : r \"\"\"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: $$ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} $$ Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Returns: angle (float): Angle between vectors BA and BC in radians or degrees. Shape: - Input: - a (np.ndarray): (2, ) - b (np.ndarray): (2, ) - c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) # arccos range is [0, pi] angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node draws keypoints and counts the number of push ups. Args: inputs (dict): Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. Note: To check the shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to [PeekingDuck API Documentation](https://peekingduck.readthedocs.io/en/stable/nodes/model.movenet.html#module-model.movenet). Returns: outputs (dict): Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. \"\"\" # get required inputs from pipeline img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] filename = inputs [ \"filename\" ] # image width, height img_size = ( img . shape [ 1 ], img . shape [ 0 ]) # frame count should not be in the if-clause self . frame_count += 1 if not self . is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ): # assume each frame has only one person; # note this bbox is from the posenet/movenet and not from yolo. the_bbox = bboxes [ 0 ] # bbox_scores are from yolo and not posenet/movenet. the_bbox_score = bbox_scores [ 0 ] if len ( bbox_scores ) > 0 else 0 x1 , _y1 , _x2 , y2 = map_bbox_to_image_coords ( the_bbox , img_size ) score_str = f \"BBox { the_bbox_score : 0.2f } \" # get bounding box confidence score and draw it at the left-bottom # (x1, y2) corner of the bounding box (offset by 30 pixels) draw_text ( img , score_str , ( x1 , y2 - 30 ), color = self . global_params_dataclass . WHITE , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1.0 , thickness = 3 , ) # assume each frame has only one person; the_keypoints = keypoints [ 0 ] the_keypoint_scores = keypoint_scores [ 0 ] # count the number of push ups self . count_push_ups ( img , img_size , the_keypoints , the_keypoint_scores ) # careful not to indent this return statement; # if the if-clause is false, then no dict will be returned and will crash the pipeline return { \"filename\" : filename , \"expected_pose\" : self . expected_pose , \"num_push_ups\" : self . num_push_ups , \"frame_count\" : self . frame_count , \"elbow_angle\" : self . elbow_angle , \"elbow_keypoint\" : self . left_elbow , \"shoulder_keypoint\" : self . left_shoulder , \"wrist_keypoint\" : self . left_wrist , }","title":"dabble/exercise_counter.py"},{"location":"exercise_counter/#outputcsv_writeryml","text":"This is a default node and we will make use of the output.csv_writer node to write the results from exercise_counter to a CSV file. A quick check at the default settings from peekingduck.configs.output.csv_writer.yml yields: default output.csv_writer.yml 1 2 3 4 5 6 input : [ \"all\" ] output : [ \"none\" ] stats_to_track : [ \"keypoints\" , \"bboxes\" , \"bbox_labels\" ] file_path : \"PeekingDuck/data/stats.csv\" logging_interval : 1 # in terms of seconds between each log We are fine with input and output as they are but need to modify the configurations from [ lines 4 - 6 ] . Since the configurations' key names are not changed, we can directly overwrite them in pipeline_config.yml as follows: pipeline_config.yml 1 2 3 4 5 6 7 8 - output.csv_writer : stats_to_track : [ \"keypoints\" , \"bboxes\" , \"bbox_labels\" , \"num_push_ups\" , \"frame_count\" , \"body_direction\" , \"elbow_angle\" , \"shoulder_keypoint\" , \"elbow_keypoint\" , \"wrist_keypoint\" , \"filename\" ] # (1) file_path : \"./stores/artifacts/push_ups_output_movenet.csv\" # (2) logging_interval : 0 # (3) The main aim is to keep track of keypoints information per frame. The file_path is set to ./store/artifacts/push_ups_output_movenet.csv . This is where the CSV file will be written to. The logging_interval is set to 0. This means that the CSV file will be written to every frame. The code block from peekingduck.pipeline.nodes.output.utils.csvlogger shows how the logging_interval logic is implemented. if ( curr_time - self . last_write ) . seconds >= self . logging_interval : self . writer . writerow ( content ) self . last_write = curr_time Info A small recap, this node is used to write the results to a CSV file and is chained after the dabble.exercise_counter node. We pass the output dict of the dabble.exercise_counter as inputs to ( stats_to_track ).","title":"output/csv_writer.yml"},{"location":"exercise_counter/#interpretation-of-csv-outputs","text":"The snippet below shows the CSV file contents. Fig 2: Push-up Counter CSV Keypoints of elbow, shoulder, wrist as well as elbow angles are recorded every frame, if some of them are not detected by the model, it will be None and recorded as an empty string in the CSV file. If any of elbow, shoulder and wrist keypoints are None , then the corresponding elbow angle will be the same as the previous frame.","title":"Interpretation of CSV outputs"},{"location":"exercise_counter/#references","text":"This is my first encounter with Pose Estimation. Here are some references that I draw inspiration from. Next-Generation Pose Detection with MoveNet and TensorFlow.js Multi-Person Pose Estimation with Mediapipe How I created the Workout Movement Counting App using Deep Learning and Optical Flow Algorithm Human Pose Classification with MoveNet and TensorFlow Lite MoveNet: Ultra fast and accurate pose detection model Deep learning approaches for workout repetition counting and validation Push-up counter using Mediapipe python Pose Classification From MediaPipe RepCounter using PoseNet Exercise Reps Counter || Pose Estimation Deep Learning Exercise Repetitions Counter Real-time Human Pose Estimation in the Browser with TensorFlow.js Fitness Camera \u2013 Turn Your Phone's Camera Into a Fitness Tracker Keypoint IDs \u21a9 Coordinate Systems \u21a9 You can read more about pipeline config in PeekingDuck's Documentation . \u21a9 There are various ways to create custom nodes. See more from the PeekingDuck documentation . \u21a9 The values for \\(S\\) and \\(E\\) are configurable, but \\(155^{\\circ}\\) and \\(90^{\\circ}\\) are good defaults. \u21a9","title":"References"},{"location":"melanoma_gradcam/","text":"The Custom Nodes This section shows how to use a custom trained model to perform inference with integration with the PeekingDuck framework. It assumes that you are already familiar with the process of creating custom nodes, covered in my other tutorial . configs/model/melanoma_classifier.yml We first see what content is inside the config file. melanoma_classifier.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 input : [ \"img\" ] output : [ \"pred_label\" , \"pred_score\" , \"gradcam_image\" ] weights_parent_dir : pytorch_models weights : { model_subdir : resnet50d , blob_file : resnet50d.zip , classes_file : melanoma_class_mapping.yml , model_file : { resnet34d : resnet34d.pt , resnet50d : resnet50d.pt , } } model_params : { model_name : resnet50d , out_features : 2 , in_channels : 3 , pretrained : false , use_meta : false } num_classes : 2 class_label_map : { 0 : \"benign\" , 1 : \"malignant\" , } model_type : resnet50d input_size : 224 mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] half : false plot_gradcam : false [ Lines 1 - 2 ] : The input takes in the built-in PeekingDuck img input and outputs the pred_label and pred_score , the former being the predicted labels from [ Lines 25 - 28 ] and the latter, the corresponding probability of the predicted label. On top of that, we also output gradcam_image which is a heatmap overlayed on the input image. [ Line 4 ] : The weights of your trained model are stored in the weights_parent_dir directory. [ Lines 5 - 13 ] : The weights key corresponds to a dictionary: model_subdir : The model_subdir is a sub-directory of weights_parent_dir directory. The name should be indicative of the model architecture. blob_file : The blob_file is the name of the file that contains the trained model weights. For example, if you stored your trained weights resnet50d.zip on Google Cloud Storage, then the blob_file necessarily should be named resnet50d.zip in order for downloader to download the weights from the cloud. classes_file : The classes_file is the name of the file that contains the mapping between the class labels and their corresponding class names. One can also define it in the current config file as well: [ Lines 25 - 28 ] . model_file : The model_file is a dictionary that maps the model architecture to the trained model weights name. For example, resnet50d.pt is the weight extracted from resnet50d.zip . This mapping is needed for us to define the model_path in detector.py . See model_path = (self.model_dir / self.config[\"weights\"][\"model_file\"][model_type]) in detector.py . [ Lines 15 - 21 ] : The model_params key corresponds to a dictionary that will be unpacked in resnet_files/model.py . [ Line 23 ] : The num_classes indicates the number of unique classes in the dataset. [ Lines 25 - 28 ] : The class_label_map key corresponds to a dictionary that maps the class labels to their corresponding class names. This should be exactly the same as classes_file in [ Line 8 ] . [ Line 30 ] : The model_type indicates the model architecture. This may be redundant since it is defined in the model_params key. [ Line 31 - 33 ] : These 3 lines are the transform parameters, used to preprocess the input image. [ Line 34 ] : The half parameter indicates whether the model is trained on a half precision or not. [ Line 35 ] : The plot_gradcam parameter indicates whether the gradcam heatmap should be plotted or not. model/melanoma_classifier.py The highlighted lines below basically does the following: Convert input image from BGR to RGB; Make a prediction on the input image using the trained model; Make a heatmap of the input image using the gradcam algorithm; Return the predicted label and the corresponding probability alongside the heatmap. model.melanoma_classifier.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Node ( AbstractNode ): \"\"\"Initializes and uses a ResNet to predict if an image frame is a melanoma or not.\"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) self . plot_gradcam : bool self . model = resnet_model . ResnetModel ( self . config ) self . input_shape = ( self . input_size , self . input_size ) def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Reads the image input and returns the predicted class label and probability. Args: inputs (dict): Dictionary with key \"img\". Returns: outputs (dict): Dictionary with keys \"pred_label\", \"pred_score\" and \"gradcam_image\". \"\"\" img = cv2 . cvtColor ( inputs [ \"img\" ], cv2 . COLOR_BGR2RGB ) reshaped_original_image = cv2 . resize ( img , self . input_shape ) prediction_dict = self . model . predict ( img ) gradcam_image = self . model . show_gradcam ( reshaped_original_image , self . plot_gradcam ) return { ** prediction_dict , \"gradcam_image\" : gradcam_image , } model/resnets/resnet_files/downloader.py This script is the same as the one in PeekingDuck's weights utils . The only difference is I changed the global variable BASE_URL : Original vs New BASE_URL = \"https://storage.googleapis.com/peekingduck/models\" # Original BASE_URL = \"https://storage.googleapis.com/reighns/peekingduck/models\" # New so that I can download the model weights from my own bucket. model/resnets/resnet_files/model.py This script contains the ResNet model structure with backbone and head. Currently, the model is hardcoded to only take in models from the famous PyTorch timm library . As this was ported over from my personal project, the name of the class is called CustomNeuralNet instead of a more indicate name such as ResNetModel . model/resnets/resnet_files/detector.py This script contains the Detector class which is used to predict melanoma. model/resnets/resnet_model.py This script contains the ResnetModel class which is used to validate configuration, loads the ResNet model through Detector , and performs inference with the method predict . pipeline_config.yml Our pipeline_config.yml file to use the input.visual node to read in the images from the melanoma_data/test directory and output the prediction information on the image. Let's walkthrough the file: pipeline_config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 nodes : - custom_nodes.input.visual : source : melanoma_data/test - custom_nodes.model.melanoma_classifier - output.csv_writer : stats_to_track : [ \"filename\" , \"pred_label\" , \"pred_score\" ] file_path : \"./stores/artifacts/resnet_melanoma.csv\" logging_interval : 0 - draw.legend : show : [ \"filename\" , \"pred_label\" , \"pred_score\" ] - custom_nodes.output.screen - output.media_writer : output_dir : \"./stores/artifacts/\" [ Lines 2 - 3 ] : Load the images from the melanoma_data/test directory. [ Line 4 ] : The custom node will initiate the trained model and perform inferences. Note it will output the predicted label, probability, and the heatmap of the input image. [ Lines 5 - 8 ] : The output.csv_writer node will write the prediction information to a csv file. The filename will be the name of the image. [ Lines 9 - 10 ] : The draw.legend node will show filename , pred_label , and pred_score on the screen. [ Line 11 ] : The custom_nodes.output.screen node will show both the original image and the heatmap on the screen. [ Lines 12 - 13 ] : The output.media_writer node will write the heatmap and the original image to the ./stores/artifacts/ directory. Interpreting the CSV Our CSV file contains the following columns: Time filename pred_label pred_score 17:12:46 ISIC_0074311.jpg benign 99.99732971 17:12:47 ISIC_0076262.jpg benign 99.99479055 17:12:49 ISIC_0098198.jpg benign 99.9946475 17:12:50 ISIC_0100550.jpg benign 99.99884367 Something to clarify, our dataset is a binary classification problem and therefore two classes are predicted with their respective \"confidence\" scores. In our model, we used a Softmax as our final layer activation function, which will return each prediction as an array of shape=(1, 2) . More concretely: The first image ISIC_0074311.jpg has a predicted label of benign and a confidence score of 99.99732971 , but behind the scenes the pred_score is [[9.999733e-01 2.673265e-05]] . If we are less pedantic and treat this as a row vector, then the first element of the array is the probability of the image being a benign image and the second element is the probability of the image being a malignant image. We choose the maximum of the array as our final prediction. Visualize Results We take a look at the images saved in our ./stores/artifacts/ directory. The below will be the ones with Grad-CAM:","title":"Melanoma Prediction with Grad-CAM"},{"location":"melanoma_gradcam/#the-custom-nodes","text":"This section shows how to use a custom trained model to perform inference with integration with the PeekingDuck framework. It assumes that you are already familiar with the process of creating custom nodes, covered in my other tutorial .","title":"The Custom Nodes"},{"location":"melanoma_gradcam/#configsmodelmelanoma_classifieryml","text":"We first see what content is inside the config file. melanoma_classifier.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 input : [ \"img\" ] output : [ \"pred_label\" , \"pred_score\" , \"gradcam_image\" ] weights_parent_dir : pytorch_models weights : { model_subdir : resnet50d , blob_file : resnet50d.zip , classes_file : melanoma_class_mapping.yml , model_file : { resnet34d : resnet34d.pt , resnet50d : resnet50d.pt , } } model_params : { model_name : resnet50d , out_features : 2 , in_channels : 3 , pretrained : false , use_meta : false } num_classes : 2 class_label_map : { 0 : \"benign\" , 1 : \"malignant\" , } model_type : resnet50d input_size : 224 mean : [ 0.485 , 0.456 , 0.406 ] std : [ 0.229 , 0.224 , 0.225 ] half : false plot_gradcam : false [ Lines 1 - 2 ] : The input takes in the built-in PeekingDuck img input and outputs the pred_label and pred_score , the former being the predicted labels from [ Lines 25 - 28 ] and the latter, the corresponding probability of the predicted label. On top of that, we also output gradcam_image which is a heatmap overlayed on the input image. [ Line 4 ] : The weights of your trained model are stored in the weights_parent_dir directory. [ Lines 5 - 13 ] : The weights key corresponds to a dictionary: model_subdir : The model_subdir is a sub-directory of weights_parent_dir directory. The name should be indicative of the model architecture. blob_file : The blob_file is the name of the file that contains the trained model weights. For example, if you stored your trained weights resnet50d.zip on Google Cloud Storage, then the blob_file necessarily should be named resnet50d.zip in order for downloader to download the weights from the cloud. classes_file : The classes_file is the name of the file that contains the mapping between the class labels and their corresponding class names. One can also define it in the current config file as well: [ Lines 25 - 28 ] . model_file : The model_file is a dictionary that maps the model architecture to the trained model weights name. For example, resnet50d.pt is the weight extracted from resnet50d.zip . This mapping is needed for us to define the model_path in detector.py . See model_path = (self.model_dir / self.config[\"weights\"][\"model_file\"][model_type]) in detector.py . [ Lines 15 - 21 ] : The model_params key corresponds to a dictionary that will be unpacked in resnet_files/model.py . [ Line 23 ] : The num_classes indicates the number of unique classes in the dataset. [ Lines 25 - 28 ] : The class_label_map key corresponds to a dictionary that maps the class labels to their corresponding class names. This should be exactly the same as classes_file in [ Line 8 ] . [ Line 30 ] : The model_type indicates the model architecture. This may be redundant since it is defined in the model_params key. [ Line 31 - 33 ] : These 3 lines are the transform parameters, used to preprocess the input image. [ Line 34 ] : The half parameter indicates whether the model is trained on a half precision or not. [ Line 35 ] : The plot_gradcam parameter indicates whether the gradcam heatmap should be plotted or not.","title":"configs/model/melanoma_classifier.yml"},{"location":"melanoma_gradcam/#modelmelanoma_classifierpy","text":"The highlighted lines below basically does the following: Convert input image from BGR to RGB; Make a prediction on the input image using the trained model; Make a heatmap of the input image using the gradcam algorithm; Return the predicted label and the corresponding probability alongside the heatmap. model.melanoma_classifier.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 class Node ( AbstractNode ): \"\"\"Initializes and uses a ResNet to predict if an image frame is a melanoma or not.\"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) self . plot_gradcam : bool self . model = resnet_model . ResnetModel ( self . config ) self . input_shape = ( self . input_size , self . input_size ) def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Reads the image input and returns the predicted class label and probability. Args: inputs (dict): Dictionary with key \"img\". Returns: outputs (dict): Dictionary with keys \"pred_label\", \"pred_score\" and \"gradcam_image\". \"\"\" img = cv2 . cvtColor ( inputs [ \"img\" ], cv2 . COLOR_BGR2RGB ) reshaped_original_image = cv2 . resize ( img , self . input_shape ) prediction_dict = self . model . predict ( img ) gradcam_image = self . model . show_gradcam ( reshaped_original_image , self . plot_gradcam ) return { ** prediction_dict , \"gradcam_image\" : gradcam_image , }","title":"model/melanoma_classifier.py"},{"location":"melanoma_gradcam/#modelresnetsresnet_filesdownloaderpy","text":"This script is the same as the one in PeekingDuck's weights utils . The only difference is I changed the global variable BASE_URL : Original vs New BASE_URL = \"https://storage.googleapis.com/peekingduck/models\" # Original BASE_URL = \"https://storage.googleapis.com/reighns/peekingduck/models\" # New so that I can download the model weights from my own bucket.","title":"model/resnets/resnet_files/downloader.py"},{"location":"melanoma_gradcam/#modelresnetsresnet_filesmodelpy","text":"This script contains the ResNet model structure with backbone and head. Currently, the model is hardcoded to only take in models from the famous PyTorch timm library . As this was ported over from my personal project, the name of the class is called CustomNeuralNet instead of a more indicate name such as ResNetModel .","title":"model/resnets/resnet_files/model.py"},{"location":"melanoma_gradcam/#modelresnetsresnet_filesdetectorpy","text":"This script contains the Detector class which is used to predict melanoma.","title":"model/resnets/resnet_files/detector.py"},{"location":"melanoma_gradcam/#modelresnetsresnet_modelpy","text":"This script contains the ResnetModel class which is used to validate configuration, loads the ResNet model through Detector , and performs inference with the method predict .","title":"model/resnets/resnet_model.py"},{"location":"melanoma_gradcam/#pipeline_configyml","text":"Our pipeline_config.yml file to use the input.visual node to read in the images from the melanoma_data/test directory and output the prediction information on the image. Let's walkthrough the file: pipeline_config.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 nodes : - custom_nodes.input.visual : source : melanoma_data/test - custom_nodes.model.melanoma_classifier - output.csv_writer : stats_to_track : [ \"filename\" , \"pred_label\" , \"pred_score\" ] file_path : \"./stores/artifacts/resnet_melanoma.csv\" logging_interval : 0 - draw.legend : show : [ \"filename\" , \"pred_label\" , \"pred_score\" ] - custom_nodes.output.screen - output.media_writer : output_dir : \"./stores/artifacts/\" [ Lines 2 - 3 ] : Load the images from the melanoma_data/test directory. [ Line 4 ] : The custom node will initiate the trained model and perform inferences. Note it will output the predicted label, probability, and the heatmap of the input image. [ Lines 5 - 8 ] : The output.csv_writer node will write the prediction information to a csv file. The filename will be the name of the image. [ Lines 9 - 10 ] : The draw.legend node will show filename , pred_label , and pred_score on the screen. [ Line 11 ] : The custom_nodes.output.screen node will show both the original image and the heatmap on the screen. [ Lines 12 - 13 ] : The output.media_writer node will write the heatmap and the original image to the ./stores/artifacts/ directory.","title":"pipeline_config.yml"},{"location":"melanoma_gradcam/#interpreting-the-csv","text":"Our CSV file contains the following columns: Time filename pred_label pred_score 17:12:46 ISIC_0074311.jpg benign 99.99732971 17:12:47 ISIC_0076262.jpg benign 99.99479055 17:12:49 ISIC_0098198.jpg benign 99.9946475 17:12:50 ISIC_0100550.jpg benign 99.99884367 Something to clarify, our dataset is a binary classification problem and therefore two classes are predicted with their respective \"confidence\" scores. In our model, we used a Softmax as our final layer activation function, which will return each prediction as an array of shape=(1, 2) . More concretely: The first image ISIC_0074311.jpg has a predicted label of benign and a confidence score of 99.99732971 , but behind the scenes the pred_score is [[9.999733e-01 2.673265e-05]] . If we are less pedantic and treat this as a row vector, then the first element of the array is the probability of the image being a benign image and the second element is the probability of the image being a malignant image. We choose the maximum of the array as our final prediction.","title":"Interpreting the CSV"},{"location":"melanoma_gradcam/#visualize-results","text":"We take a look at the images saved in our ./stores/artifacts/ directory. The below will be the ones with Grad-CAM:","title":"Visualize Results"},{"location":"pkd/","text":"Source Code Tracing peekingduck run : If you check entry_points.txt , the entry point is indeed peekingduck pointing to the package peekingduck.cli script. The cli after the colon : means that we will look at the decorator commands @cli.command() . setup.cfg | entry_points.txt 1 2 3 4 # https://github.com/aimakerspace/PeekingDuck/blob/dev/setup.cfg [options.entry_points] console_scripts = peekingduck = peekingduck.cli:cli Consequently, when peekingduck run is called, it goes through the following files: [ peekingduck . cli . run () - Line 1 ]: The decorator @cli.command() is called. No arguments were passed on the command line and therefore the default arguments be taken from @click.option() . [ peekingduck . cli . run () - Lines 2 - 26 ]: The decorator @click.option() takes in various things such as default arguments for the function. For example, the default argument for node_config is defaulted to None . [ peekingduck . cli . run () - Lines 27 - 33 ]: This is the function that is called when peekingduck run is called. Let us check the input arguments here: config_path : If not provided, it defaults to pipeline_config.yml . The new variable is called pipeline_config_path . log_level : INFO level. node_config : None as no custom arguments provided. num_iter : None nodes_parent_dir : Points to src , the folder that was created by default! Override if you want to rename your folder. [ peekingduck . cli . run () - Lines 38 - 46 ]: We set pipeline_config_path to Path(config_path) . This variable holds the path to our pipeline_config.yml file. [ peekingduck . cli . run () - Lines 49 - 55 ]: The Runner class is called, we can trace the code in the peekingduck.runner.py . We will now go to the peekingduck.runner.py file. Note that I highlighted line 54 to set nodes=None for my own clarity purposes. [ peekingduck . runner . Runner () - Lines 27 - 34 ]: We see that Runner() class takes in the following arguments: pipeline_path: Path = None config_updates_cli: str = None custom_nodes_parent_subdir: str = None num_iter: int = None nodes: List[AbstractNode] = None All of which were passed in to the Runner class in [ peekingduck . cli . run () - Lines 49 - 55 ] . [ peekingduck . runner . Runner () - Lines 41 - 45 ]: The Runner() class took in pipeline_path , config_updates_cli and custom_nodes_parent_subdir and thus will go into the elif of the if-elif-else statement. [ peekingduck . runner . Runner () - Lines 47 - 51 ]: self.node_loader will call DeclarativeLoader which is a helper class to create the final Pipeline <peekingduck.pipeline.pipeline.Pipeline> object. Let us see what it does at a high level. Take note that at this stage we are just initializing the the DeclarativeLoader class; consequently, we should look what happens in its __init__ method. [ peekingduck . declarative_loader . DeclarativeLoader () - Lines 53 - 57 ]: We take a look at some key attributes and methods of the DeclarativeLoader class. [ peekingduck . declarative_loader . DeclarativeLoader () - Line 33 ]: The attribute self.node_list = self._load_node_list(pipeline_path) is instantiated. We take a look at one of the important attributes of the DeclarativeLoader class, node_list . This is a NodeList object (defined in the same file) that has the attribute nodes which gives us the following. This means they loaded all the config in pipeline_config.yml into a list of dict. self.node_list : It returns NodeList(upgraded_nodes) as a NodeList object, an abstract object defined in [ peekingduck . declarative_loader . DeclarativeLoader () - Line 214 - 240 ] . self.node_list.nodes : List of Dict - basically loads all config from pipeline_config.yml into a list of dict. [ { \"input.visual\" : { \"source\" : \"https://storage.googleapis.com/reighns/peekingduck/videos/wave.mp4\" } }, { \"model.yolo\" : { \"model_type\" : \"v4tiny\" , \"iou_threshold\" : 0.1 , \"score_threshold\" : 0.1 , \"detect_ids\" : [ \"person\" ], \"num_classes\" : 1 , } }, { \"model.posenet\" : { \"model_type\" : \"resnet\" , \"resolution\" : { \"height\" : 224 , \"width\" : 224 }, \"score_threshold\" : 0.05 , } }, \"dabble.fps\" , \"custom_nodes.dabble.exercise_counter\" , \"draw.poses\" , \"model.mtcnn\" , \"draw.mosaic_bbox\" , { \"draw.legend\" : { \"show\" : [ \"fps\" ]}}, \"output.screen\" , ] [ peekingduck . runner . Runner () - Line 52 ]: Creates self.pipeline which calls the get_pipeline() method in [ peekingduck . declarative_loader . DeclarativeLoader () - Line 199 ] . [ peekingduck . declarative_loader . DeclarativeLoader () - Line 204 ]: instantiated_nodes = self._instantiate_nodes() is where we create all the nodes. I'd imagine the nodes are created by looping over the list of dict just now, and for each node dict in the list, we create the AbstractNode class from it. It may be useful to check what Pipeline contains! Let us hop over to from peekingduck.pipeline.pipeline.Pipeline . Going into from peekingduck.pipeline.pipeline import Pipeline object, we see that the Pipeline object holds an attribute nodes , which unsurprisingly, refers to a list of AbstractNode object (i.e. List[AbstractNode] ). [ peekingduck . cli . run () - Line 58 ]: We are finally back to the last line of the peekingduck.cli.run() function. We call the Runner.run() method. [ peekingduck . runner . Runner () - Line 78 ]: Start of while loop with self.pipeline.terminate as False at first. [ peekingduck . runner . Runner () - Line 79 ]: Loop over self.pipeline.nodes , recall that self.pipeline.nodes is a list of AbstractNode objects. [ peekingduck . runner . Runner () - Line 83 - 84 ]: Terminating conditions. [ peekingduck . runner . Runner () - Line 91 - 95 ]: Update inputs dict if we can find a key in self.pipeline.data . This part was confusing at first since the first iteration the self.pipeline.data is an empty dict. Upon checking, it turns out that the first Node is input.visual which takes in input as None and output as output: [\"img\", \"filename\", \"pipeline_end\", \"saved_video_fps\"] . Therefore in the first iteration, there won't be any key and thus inputs will be {} but outputs from [ peekingduck . runner . Runner () - Line 103 ] will be {\"img\": ..., \"filename\": ..., \"pipeline_end\": ..., \"saved_video_fps\": ...} . Consequently, self.pipeline.data will update the dictionary inputs with the outputs from the first Node . It will keep chaining on till the last node. peekingduck.cli.run() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @cli . command () @click . option ( \"--config_path\" , default = None , type = click . Path (), help = ( \"List of nodes to run. None assumes pipeline_config.yml at current working directory\" ), ) @click . option ( \"--log_level\" , default = \"info\" , help = \"\"\"Modify log level {\"critical\", \"error\", \"warning\", \"info\", \"debug\"}\"\"\" , ) @click . option ( \"--node_config\" , default = \"None\" , help = \"\"\"Modify node configs by wrapping desired configs in a JSON string. \\n Example: --node_config '{\"node_name\": {\"param_1\": var_1}}'\"\"\" , ) @click . option ( \"--num_iter\" , default = None , type = int , help = \"Stop pipeline after running this number of iterations\" , ) def run ( config_path : str , log_level : str , node_config : str , num_iter : int , nodes_parent_dir : str = \"src\" , ) -> None : \"\"\"Runs PeekingDuck\"\"\" LoggerSetup . set_log_level ( log_level ) if config_path is None : curr_dir = _get_cwd () if ( curr_dir / \"pipeline_config.yml\" ) . is_file (): config_path = curr_dir / \"pipeline_config.yml\" elif ( curr_dir / \"run_config.yml\" ) . is_file (): config_path = curr_dir / \"run_config.yml\" else : config_path = curr_dir / \"pipeline_config.yml\" pipeline_config_path = Path ( config_path ) start_time = perf_counter () runner = Runner ( pipeline_path = pipeline_config_path , config_updates_cli = node_config , custom_nodes_parent_subdir = nodes_parent_dir , num_iter = num_iter , nodes = None ) end_time = perf_counter () logger . debug ( f \"Startup time = { end_time - start_time : .2f } sec\" ) runner . run () peekingduck.runner.Runner() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class Runner : \"\"\"The runner class for creation of pipeline using declared/given nodes. The runner class uses the provided configurations to setup a node pipeline which is used to run inference. Args: pipeline_path (:obj:`pathlib.Path` | :obj:`None`): If a path to *pipeline_config.yml* is provided, uses :py:class:`DeclarativeLoader <peekingduck.declarative_loader.DeclarativeLoader>` to load the YAML file according to PeekingDuck's specified schema to obtain the declared nodes that would be sequentially initialized and used to create the pipeline for running inference. config_updates_cli (:obj:`str` | :obj:`None`): Configuration changes passed as part of the CLI command used to modify the node configurations directly from CLI. custom_nodes_parent_subdir (:obj:`str` | :obj:`None`): Relative path to a folder which contains custom nodes that users have created to be used with PeekingDuck. For more information on using custom nodes, please refer to `Getting Started <getting_started/03_custom_nodes.html>`_. num_iter (int): Stop pipeline after running this number of iterations nodes (:obj:`List[AbstractNode]` | :obj:`None`): If a list of nodes is provided, initialize by the node stack directly. \"\"\" def __init__ ( # pylint: disable=too-many-arguments self , pipeline_path : Path = None , config_updates_cli : str = None , custom_nodes_parent_subdir : str = None , num_iter : int = None , nodes : List [ AbstractNode ] = None , ) -> None : self . logger = logging . getLogger ( __name__ ) try : if nodes : # instantiated_nodes is created differently when given nodes self . pipeline = Pipeline ( nodes ) elif ( pipeline_path and config_updates_cli and custom_nodes_parent_subdir ): # create Graph to run self . node_loader = DeclarativeLoader ( pipeline_path , config_updates_cli , custom_nodes_parent_subdir , ) self . pipeline = self . node_loader . get_pipeline () else : raise ValueError ( \"Arguments error! Pass in either nodes to load directly via \" \"Pipeline or pipeline_path, config_updates_cli, and \" \"custom_nodes_parent_subdir to load via DeclarativeLoader.\" ) except ValueError as error : self . logger . error ( str ( error )) sys . exit ( 1 ) if RequirementChecker . n_update > 0 : self . logger . warning ( f \" { RequirementChecker . n_update } package\" f \" { 's' * int ( RequirementChecker . n_update > 1 ) } updated. \" \"Please rerun for the updates to take effect.\" ) sys . exit ( 3 ) if num_iter is None or num_iter <= 0 : self . num_iter = 0 else : self . num_iter = num_iter self . logger . info ( f \"Run pipeline for { num_iter } iterations\" ) def run ( self ) -> None : # pylint: disable=too-many-branches \"\"\"execute single or continuous inference\"\"\" num_iter = 0 while not self . pipeline . terminate : for node in self . pipeline . nodes : if num_iter == 0 : # report node setup times at first iteration self . logger . debug ( f \"First iteration: setup { node . name } ...\" ) node_start_time = perf_counter () if self . pipeline . data . get ( \"pipeline_end\" , False ): self . pipeline . terminate = True if \"pipeline_end\" not in node . inputs : continue if \"all\" in node . inputs : inputs = copy . deepcopy ( self . pipeline . data ) else : inputs = { key : self . pipeline . data [ key ] for key in node . inputs if key in self . pipeline . data } if hasattr ( node , \"optional_inputs\" ): for key in node . optional_inputs : # The nodes will not receive inputs with the optional # key if it's not found upstream if key in self . pipeline . data : inputs [ key ] = self . pipeline . data [ key ] outputs = node . run ( inputs ) self . pipeline . data . update ( outputs ) if num_iter == 0 : node_end_time = perf_counter () self . logger . debug ( f \" { node . name } setup time = { node_end_time - node_start_time : .2f } sec\" ) num_iter += 1 if self . num_iter > 0 and num_iter >= self . num_iter : self . logger . info ( f \"Stopping pipeline after { num_iter } iterations\" ) break # clean up nodes with threads for node in self . pipeline . nodes : if node . name . endswith ( \".visual\" ): node . release_resources () def get_pipeline ( self ) -> NodeList : \"\"\"Retrieves run configuration. Returns: (:obj:`Dict`): Run configurations being used by runner. \"\"\" return self . node_loader . node_list peekingduck.declarative_loader.DeclarativeLoader() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class DeclarativeLoader : # pylint: disable=too-few-public-methods \"\"\"A helper class to create :py:class:`Pipeline <peekingduck.pipeline.pipeline.Pipeline>`. The declarative loader class creates the specified nodes according to any modifications provided in the configs and returns the pipeline needed for inference. Args: pipeline_path (:obj:`pathlib.Path`): Path to a YAML file that declares the node sequence to be used in the pipeline. config_updates_cli (:obj:`str`): Stringified nested dictionaries of configuration changes passed as part of CLI command. Used to modify the node configurations directly from the CLI. custom_nodes_parent_subdir (:obj:`str`): Relative path to parent folder which contains custom nodes that users have created to be used with PeekingDuck. For more information on using custom nodes, please refer to `Getting Started <getting_started/03_custom_nodes.html>`_. \"\"\" def __init__ ( self , pipeline_path : Path , config_updates_cli : str , custom_nodes_parent_subdir : str , ) -> None : self . logger = logging . getLogger ( __name__ ) self . pkd_base_dir = Path ( __file__ ) . resolve () . parent self . config_loader = ConfigLoader ( self . pkd_base_dir ) self . node_list = self . _load_node_list ( pipeline_path ) self . config_updates_cli = ast . literal_eval ( config_updates_cli ) custom_nodes_name = self . _get_custom_name_from_node_list () if custom_nodes_name is not None : custom_nodes_dir = ( Path . cwd () / custom_nodes_parent_subdir / custom_nodes_name ) self . custom_config_loader = ConfigLoader ( custom_nodes_dir ) sys . path . append ( custom_nodes_parent_subdir ) self . custom_nodes_dir = custom_nodes_dir def _load_node_list ( self , pipeline_path : Path ) -> \"NodeList\" : \"\"\"Loads a list of nodes from pipeline_path.yml\"\"\" # dotw 2022-03-17: Temporary helper methods def deprecation_warning ( name : str , config : Union [ str , Dict [ str , Any ]] ) -> None : self . logger . warning ( f \"` { name } ` deprecated, replaced by `input.visual`\" ) self . logger . warning ( f \"convert ` { name } ` to `input.visual`: { config } \" ) with open ( pipeline_path ) as node_yml : data = yaml . safe_load ( node_yml ) if not isinstance ( data , dict ) or \"nodes\" not in data : raise ValueError ( f \" { pipeline_path } has an invalid structure. \" \"Missing top-level 'nodes' key.\" ) nodes = data [ \"nodes\" ] if nodes is None : raise ValueError ( f \" { pipeline_path } does not contain any nodes!\" ) upgraded_nodes = [] for node in nodes : if isinstance ( node , str ): if node in [ \"input.live\" , \"input.recorded\" ]: deprecation_warning ( node , \"input.visual\" ) if node == \"input.live\" : node = { \"input.visual\" : { \"source\" : 0 }} else : self . logger . error ( \"input.recorded with no parameters error!\" ) node = \"input.visual\" else : if \"input.live\" in node : node_config = node . pop ( \"input.live\" ) if \"input_source\" in node_config : node_config [ \"source\" ] = node_config . pop ( \"input_source\" ) node [ \"input.visual\" ] = node_config deprecation_warning ( \"input.live\" , node_config ) if \"input.recorded\" in node : node_config = node . pop ( \"input.recorded\" ) if \"input_dir\" in node_config : node_config [ \"source\" ] = node_config . pop ( \"input_dir\" ) node [ \"input.visual\" ] = node_config deprecation_warning ( \"input.recorded\" , node_config ) upgraded_nodes . append ( node ) self . logger . info ( \"Successfully loaded pipeline file.\" ) return NodeList ( upgraded_nodes ) def _get_custom_name_from_node_list ( self ) -> Any : custom_name = None for node_str , _ in self . node_list : node_type = node_str . split ( \".\" )[ 0 ] if node_type not in PEEKINGDUCK_NODE_TYPES : custom_name = node_type break return custom_name def _instantiate_nodes ( self ) -> List [ AbstractNode ]: \"\"\"Given a list of imported nodes, instantiate nodes\"\"\" instantiated_nodes = [] for node_str , config_updates_yml in self . node_list : node_str_split = node_str . split ( \".\" ) self . logger . info ( f \"Initializing { node_str } node...\" ) if len ( node_str_split ) == 3 : # convert windows/linux filepath to a module path path_to_node = f \" { self . custom_nodes_dir . name } .\" node_name = \".\" . join ( node_str_split [ - 2 :]) instantiated_node = self . _init_node ( path_to_node , node_name , self . custom_config_loader , config_updates_yml , ) else : path_to_node = \"peekingduck.pipeline.nodes.\" instantiated_node = self . _init_node ( path_to_node , node_str , self . config_loader , config_updates_yml , ) instantiated_nodes . append ( instantiated_node ) return instantiated_nodes def _init_node ( self , path_to_node : str , node_name : str , config_loader : ConfigLoader , config_updates_yml : Optional [ Dict [ str , Any ]], ) -> AbstractNode : \"\"\"Imports node to filepath and initializes node with config.\"\"\" node = importlib . import_module ( path_to_node + node_name ) config = config_loader . get ( node_name ) # First, override default configs with values from pipeline_config.yml if config_updates_yml is not None : config = self . _edit_config ( config , config_updates_yml , node_name ) # Second, override configs again with values from cli if self . config_updates_cli is not None : if node_name in self . config_updates_cli . keys (): config = self . _edit_config ( config , self . config_updates_cli [ node_name ], node_name ) return node . Node ( config ) def _edit_config ( self , dict_orig : Dict [ str , Any ], dict_update : Dict [ str , Any ], node_name : str , ) -> Dict [ str , Any ]: \"\"\"Update value of a nested dictionary of varying depth using recursion\"\"\" for key , value in dict_update . items (): if isinstance ( value , collections . abc . Mapping ): dict_orig [ key ] = self . _edit_config ( dict_orig . get ( key , {}), value , node_name # type: ignore ) else : if key not in dict_orig : self . logger . warning ( f \"Config for node { node_name } does not have the key: { key } \" ) else : if key == \"detect_ids\" : key , value = obj_det_change_class_name_to_id ( node_name , key , value ) dict_orig [ key ] = value self . logger . info ( f \"Config for node { node_name } is updated to: ' { key } ': { value } \" ) return dict_orig def get_pipeline ( self ) -> Pipeline : \"\"\"Returns a compiled :py:class:`Pipeline <peekingduck.pipeline.pipeline.Pipeline>` for PeekingDuck :py:class:`Runner <peekingduck.runner.Runner>` to execute. \"\"\" instantiated_nodes = self . _instantiate_nodes () try : return Pipeline ( instantiated_nodes ) except ValueError as error : self . logger . error ( str ( error )) sys . exit ( 1 ) class NodeList : \"\"\"Iterator class to return node string and node configs (if any) from the nodes declared in the run config file. \"\"\" def __init__ ( self , nodes : List [ Union [ Dict [ str , Any ], str ]]) -> None : self . nodes = nodes self . length = len ( nodes ) def __iter__ ( self ) -> Iterator [ Tuple [ str , Optional [ Dict [ str , Any ]]]]: self . current = - 1 return self def __next__ ( self ) -> Tuple [ str , Optional [ Dict [ str , Any ]]]: self . current += 1 if self . current >= self . length : raise StopIteration node_item = self . nodes [ self . current ] if isinstance ( node_item , dict ): node_str = next ( iter ( node_item )) config_updates = node_item [ node_str ] else : node_str = node_item config_updates = None return node_str , config_updates The Abstract Node Class abstract node class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from typing import Any , Dict from peekingduck.pipeline.nodes.node import AbstractNode class Node ( AbstractNode ): \"\"\"This is a template class of how to write a node for PeekingDuck. Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) # initialize/load any configs and models here # configs can be called by self.<config_name> e.g. self.filepath # self.logger.info(f\"model loaded with configs: config\") def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node does ___. Args: inputs (dict): Dictionary with keys \"__\", \"__\". Returns: outputs (dict): Dictionary with keys \"__\". \"\"\" # result = do_something(inputs[\"in1\"], inputs[\"in2\"]) # outputs = {\"out1\": result} # return outputs ABC Class Notice that all custom nodes are derived from the AbstractNode class from peekingduck.pipeline.nodes.node import AbstractNode . This class is the base class for all custom nodes, as we will soon see. @abstractmethod : This is a decorator which indicates that the method is abstract. More concretely, once a class inherits from AbstractNode , it must implement all the abstract methods. In this case, we need to implement the run method in each custom node. abstract method 1 2 3 4 @abstractmethod def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"abstract method needed for running node\"\"\" raise NotImplementedError ( \"This method needs to be implemented\" ) The Run Method As we previously see, the custom nodes are derived from the AbstractNode class. Each custom node must implement the run method. A brief check tells me that the output of the run method is a dictionary of strings and values. This is reasonable since we can see from line 133 in runner.py that outputs = node.run(inputs) , and outputs is indeed a dictionary of strings and values. We can also find out what keys the output hold by going to visual.yml and looking at the output key. The logic holds for other default nodes. Problems Chaining Yolo and Posenet I set all thresholds from Yolo and Posenet to be \\(0\\) to avoid any quality check on the bboxes so that I can attempt to debug the issue. The issue turns out to be pretty confusing, when inputs were printed in the dabble.exercise_counter , it turns out that some bboxes were populated but the bbox_scores were not. This should not be the case since a bijective relationship should be formed. Upon further digging, here is what I found: Using yolov4tiny , the model did not manage to predict the person class. Thus outputs dict consisting of bboxes, bbox_labels, bbox_scores were empty in the model.yolo portion. We turn our attention to model.posenet , it turns out that bboxes were again predicted at the Posenet level. This tells us half the story, because bboxes variable is now indeed populated (if Posenet says yes) but bbox_scores variable is still empty. bboxes , keypoints , keypoint_scores , keypoint_conns = self . model . predict ( inputs [ \"img\" ] ) bbox_labels = np . array ([ \"person\" ] * len ( bboxes )) bboxes = np . clip ( bboxes , 0 , 1 ) outputs = { \"bboxes\" : bboxes , \"keypoints\" : keypoints , \"keypoint_scores\" : keypoint_scores , \"keypoint_conns\" : keypoint_conns , \"bbox_labels\" : bbox_labels , } Consequently, when unpacking in the dabble.exercise_counter.py , img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] The bboxes are not empty but bbox_scores is, resulting in error. A bit more digging into why the bboxes are returned from Posenet. We take a look into self.model = posenet_model.PoseNetModel(self.config) which points to model.posenetv1.posenet_model , and the predict method points to Predictor class in model.posenetv1.postnet_files.predictor . It turns out the bbox is derived from bbox = self._get_bbox_of_one_pose(pose_coords, pose_masks) and these are actually the bounding boxes of the keypoints . So if there are 17 pairs of keypoints, we simply find the max corners of them. (i.e. imagine the keypoints depicts a human skeleton and we enclose them with bboxes). Something worth noting is in model.posenet , bbox_labels = np.array([\"person\"] * len(bboxes)) was coded to depict person . Need to add a clause to catch when yolo does not predict anything handling empty bbox_scores . I believe the purpose of chaining yolo then posenet is to let yolo get the person's bounding box coordinates first, then chain the cropped person to posenet for keypoint predictions... KIV first. Created debug file for yolo , posenet and exercise_counter as v4tiny seem to return errors on index out of range. Turns out yolov4tiny is not capable of detecting human sometimes, creating empty bboxes and scores, but get overwritten by bboxes found in posenet , which is a bit different from the bboxes in yolo. In the pipeline where we chain posenet after yolo , I created another debug file before exercise_counter and confirmed that if both yolo and posenet have bbox values, the latter posenet will overwrite the one from yolo . This setting is nodes : - input . visual : source : https : // storage . googleapis . com / reighns / peekingduck / videos / push_ups . mp4 - model . yolo : model_type : \"v4\" # \"v4tiny\" iou_threshold : 0.1 score_threshold : 0.1 detect_ids : [ \"person\" ] # [0] num_classes : 1 - custom_nodes . dabble . debug_yolo - model . posenet : model_type : \"resnet\" resolution : { height : 224 , width : 224 } score_threshold : 0.05 - custom_nodes . dabble . debug_posenet - dabble . fps - custom_nodes . dabble . debug_exercise_counter - custom_nodes . dabble . exercise_counter - draw . poses # - model.mtcnn # - draw.mosaic_bbox - draw . legend : show : [ \"fps\" ] - output . screen","title":"Pkd"},{"location":"pkd/#source-code-tracing","text":"peekingduck run : If you check entry_points.txt , the entry point is indeed peekingduck pointing to the package peekingduck.cli script. The cli after the colon : means that we will look at the decorator commands @cli.command() . setup.cfg | entry_points.txt 1 2 3 4 # https://github.com/aimakerspace/PeekingDuck/blob/dev/setup.cfg [options.entry_points] console_scripts = peekingduck = peekingduck.cli:cli Consequently, when peekingduck run is called, it goes through the following files: [ peekingduck . cli . run () - Line 1 ]: The decorator @cli.command() is called. No arguments were passed on the command line and therefore the default arguments be taken from @click.option() . [ peekingduck . cli . run () - Lines 2 - 26 ]: The decorator @click.option() takes in various things such as default arguments for the function. For example, the default argument for node_config is defaulted to None . [ peekingduck . cli . run () - Lines 27 - 33 ]: This is the function that is called when peekingduck run is called. Let us check the input arguments here: config_path : If not provided, it defaults to pipeline_config.yml . The new variable is called pipeline_config_path . log_level : INFO level. node_config : None as no custom arguments provided. num_iter : None nodes_parent_dir : Points to src , the folder that was created by default! Override if you want to rename your folder. [ peekingduck . cli . run () - Lines 38 - 46 ]: We set pipeline_config_path to Path(config_path) . This variable holds the path to our pipeline_config.yml file. [ peekingduck . cli . run () - Lines 49 - 55 ]: The Runner class is called, we can trace the code in the peekingduck.runner.py . We will now go to the peekingduck.runner.py file. Note that I highlighted line 54 to set nodes=None for my own clarity purposes. [ peekingduck . runner . Runner () - Lines 27 - 34 ]: We see that Runner() class takes in the following arguments: pipeline_path: Path = None config_updates_cli: str = None custom_nodes_parent_subdir: str = None num_iter: int = None nodes: List[AbstractNode] = None All of which were passed in to the Runner class in [ peekingduck . cli . run () - Lines 49 - 55 ] . [ peekingduck . runner . Runner () - Lines 41 - 45 ]: The Runner() class took in pipeline_path , config_updates_cli and custom_nodes_parent_subdir and thus will go into the elif of the if-elif-else statement. [ peekingduck . runner . Runner () - Lines 47 - 51 ]: self.node_loader will call DeclarativeLoader which is a helper class to create the final Pipeline <peekingduck.pipeline.pipeline.Pipeline> object. Let us see what it does at a high level. Take note that at this stage we are just initializing the the DeclarativeLoader class; consequently, we should look what happens in its __init__ method. [ peekingduck . declarative_loader . DeclarativeLoader () - Lines 53 - 57 ]: We take a look at some key attributes and methods of the DeclarativeLoader class. [ peekingduck . declarative_loader . DeclarativeLoader () - Line 33 ]: The attribute self.node_list = self._load_node_list(pipeline_path) is instantiated. We take a look at one of the important attributes of the DeclarativeLoader class, node_list . This is a NodeList object (defined in the same file) that has the attribute nodes which gives us the following. This means they loaded all the config in pipeline_config.yml into a list of dict. self.node_list : It returns NodeList(upgraded_nodes) as a NodeList object, an abstract object defined in [ peekingduck . declarative_loader . DeclarativeLoader () - Line 214 - 240 ] . self.node_list.nodes : List of Dict - basically loads all config from pipeline_config.yml into a list of dict. [ { \"input.visual\" : { \"source\" : \"https://storage.googleapis.com/reighns/peekingduck/videos/wave.mp4\" } }, { \"model.yolo\" : { \"model_type\" : \"v4tiny\" , \"iou_threshold\" : 0.1 , \"score_threshold\" : 0.1 , \"detect_ids\" : [ \"person\" ], \"num_classes\" : 1 , } }, { \"model.posenet\" : { \"model_type\" : \"resnet\" , \"resolution\" : { \"height\" : 224 , \"width\" : 224 }, \"score_threshold\" : 0.05 , } }, \"dabble.fps\" , \"custom_nodes.dabble.exercise_counter\" , \"draw.poses\" , \"model.mtcnn\" , \"draw.mosaic_bbox\" , { \"draw.legend\" : { \"show\" : [ \"fps\" ]}}, \"output.screen\" , ] [ peekingduck . runner . Runner () - Line 52 ]: Creates self.pipeline which calls the get_pipeline() method in [ peekingduck . declarative_loader . DeclarativeLoader () - Line 199 ] . [ peekingduck . declarative_loader . DeclarativeLoader () - Line 204 ]: instantiated_nodes = self._instantiate_nodes() is where we create all the nodes. I'd imagine the nodes are created by looping over the list of dict just now, and for each node dict in the list, we create the AbstractNode class from it. It may be useful to check what Pipeline contains! Let us hop over to from peekingduck.pipeline.pipeline.Pipeline . Going into from peekingduck.pipeline.pipeline import Pipeline object, we see that the Pipeline object holds an attribute nodes , which unsurprisingly, refers to a list of AbstractNode object (i.e. List[AbstractNode] ). [ peekingduck . cli . run () - Line 58 ]: We are finally back to the last line of the peekingduck.cli.run() function. We call the Runner.run() method. [ peekingduck . runner . Runner () - Line 78 ]: Start of while loop with self.pipeline.terminate as False at first. [ peekingduck . runner . Runner () - Line 79 ]: Loop over self.pipeline.nodes , recall that self.pipeline.nodes is a list of AbstractNode objects. [ peekingduck . runner . Runner () - Line 83 - 84 ]: Terminating conditions. [ peekingduck . runner . Runner () - Line 91 - 95 ]: Update inputs dict if we can find a key in self.pipeline.data . This part was confusing at first since the first iteration the self.pipeline.data is an empty dict. Upon checking, it turns out that the first Node is input.visual which takes in input as None and output as output: [\"img\", \"filename\", \"pipeline_end\", \"saved_video_fps\"] . Therefore in the first iteration, there won't be any key and thus inputs will be {} but outputs from [ peekingduck . runner . Runner () - Line 103 ] will be {\"img\": ..., \"filename\": ..., \"pipeline_end\": ..., \"saved_video_fps\": ...} . Consequently, self.pipeline.data will update the dictionary inputs with the outputs from the first Node . It will keep chaining on till the last node. peekingduck.cli.run() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @cli . command () @click . option ( \"--config_path\" , default = None , type = click . Path (), help = ( \"List of nodes to run. None assumes pipeline_config.yml at current working directory\" ), ) @click . option ( \"--log_level\" , default = \"info\" , help = \"\"\"Modify log level {\"critical\", \"error\", \"warning\", \"info\", \"debug\"}\"\"\" , ) @click . option ( \"--node_config\" , default = \"None\" , help = \"\"\"Modify node configs by wrapping desired configs in a JSON string. \\n Example: --node_config '{\"node_name\": {\"param_1\": var_1}}'\"\"\" , ) @click . option ( \"--num_iter\" , default = None , type = int , help = \"Stop pipeline after running this number of iterations\" , ) def run ( config_path : str , log_level : str , node_config : str , num_iter : int , nodes_parent_dir : str = \"src\" , ) -> None : \"\"\"Runs PeekingDuck\"\"\" LoggerSetup . set_log_level ( log_level ) if config_path is None : curr_dir = _get_cwd () if ( curr_dir / \"pipeline_config.yml\" ) . is_file (): config_path = curr_dir / \"pipeline_config.yml\" elif ( curr_dir / \"run_config.yml\" ) . is_file (): config_path = curr_dir / \"run_config.yml\" else : config_path = curr_dir / \"pipeline_config.yml\" pipeline_config_path = Path ( config_path ) start_time = perf_counter () runner = Runner ( pipeline_path = pipeline_config_path , config_updates_cli = node_config , custom_nodes_parent_subdir = nodes_parent_dir , num_iter = num_iter , nodes = None ) end_time = perf_counter () logger . debug ( f \"Startup time = { end_time - start_time : .2f } sec\" ) runner . run () peekingduck.runner.Runner() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class Runner : \"\"\"The runner class for creation of pipeline using declared/given nodes. The runner class uses the provided configurations to setup a node pipeline which is used to run inference. Args: pipeline_path (:obj:`pathlib.Path` | :obj:`None`): If a path to *pipeline_config.yml* is provided, uses :py:class:`DeclarativeLoader <peekingduck.declarative_loader.DeclarativeLoader>` to load the YAML file according to PeekingDuck's specified schema to obtain the declared nodes that would be sequentially initialized and used to create the pipeline for running inference. config_updates_cli (:obj:`str` | :obj:`None`): Configuration changes passed as part of the CLI command used to modify the node configurations directly from CLI. custom_nodes_parent_subdir (:obj:`str` | :obj:`None`): Relative path to a folder which contains custom nodes that users have created to be used with PeekingDuck. For more information on using custom nodes, please refer to `Getting Started <getting_started/03_custom_nodes.html>`_. num_iter (int): Stop pipeline after running this number of iterations nodes (:obj:`List[AbstractNode]` | :obj:`None`): If a list of nodes is provided, initialize by the node stack directly. \"\"\" def __init__ ( # pylint: disable=too-many-arguments self , pipeline_path : Path = None , config_updates_cli : str = None , custom_nodes_parent_subdir : str = None , num_iter : int = None , nodes : List [ AbstractNode ] = None , ) -> None : self . logger = logging . getLogger ( __name__ ) try : if nodes : # instantiated_nodes is created differently when given nodes self . pipeline = Pipeline ( nodes ) elif ( pipeline_path and config_updates_cli and custom_nodes_parent_subdir ): # create Graph to run self . node_loader = DeclarativeLoader ( pipeline_path , config_updates_cli , custom_nodes_parent_subdir , ) self . pipeline = self . node_loader . get_pipeline () else : raise ValueError ( \"Arguments error! Pass in either nodes to load directly via \" \"Pipeline or pipeline_path, config_updates_cli, and \" \"custom_nodes_parent_subdir to load via DeclarativeLoader.\" ) except ValueError as error : self . logger . error ( str ( error )) sys . exit ( 1 ) if RequirementChecker . n_update > 0 : self . logger . warning ( f \" { RequirementChecker . n_update } package\" f \" { 's' * int ( RequirementChecker . n_update > 1 ) } updated. \" \"Please rerun for the updates to take effect.\" ) sys . exit ( 3 ) if num_iter is None or num_iter <= 0 : self . num_iter = 0 else : self . num_iter = num_iter self . logger . info ( f \"Run pipeline for { num_iter } iterations\" ) def run ( self ) -> None : # pylint: disable=too-many-branches \"\"\"execute single or continuous inference\"\"\" num_iter = 0 while not self . pipeline . terminate : for node in self . pipeline . nodes : if num_iter == 0 : # report node setup times at first iteration self . logger . debug ( f \"First iteration: setup { node . name } ...\" ) node_start_time = perf_counter () if self . pipeline . data . get ( \"pipeline_end\" , False ): self . pipeline . terminate = True if \"pipeline_end\" not in node . inputs : continue if \"all\" in node . inputs : inputs = copy . deepcopy ( self . pipeline . data ) else : inputs = { key : self . pipeline . data [ key ] for key in node . inputs if key in self . pipeline . data } if hasattr ( node , \"optional_inputs\" ): for key in node . optional_inputs : # The nodes will not receive inputs with the optional # key if it's not found upstream if key in self . pipeline . data : inputs [ key ] = self . pipeline . data [ key ] outputs = node . run ( inputs ) self . pipeline . data . update ( outputs ) if num_iter == 0 : node_end_time = perf_counter () self . logger . debug ( f \" { node . name } setup time = { node_end_time - node_start_time : .2f } sec\" ) num_iter += 1 if self . num_iter > 0 and num_iter >= self . num_iter : self . logger . info ( f \"Stopping pipeline after { num_iter } iterations\" ) break # clean up nodes with threads for node in self . pipeline . nodes : if node . name . endswith ( \".visual\" ): node . release_resources () def get_pipeline ( self ) -> NodeList : \"\"\"Retrieves run configuration. Returns: (:obj:`Dict`): Run configurations being used by runner. \"\"\" return self . node_loader . node_list peekingduck.declarative_loader.DeclarativeLoader() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 class DeclarativeLoader : # pylint: disable=too-few-public-methods \"\"\"A helper class to create :py:class:`Pipeline <peekingduck.pipeline.pipeline.Pipeline>`. The declarative loader class creates the specified nodes according to any modifications provided in the configs and returns the pipeline needed for inference. Args: pipeline_path (:obj:`pathlib.Path`): Path to a YAML file that declares the node sequence to be used in the pipeline. config_updates_cli (:obj:`str`): Stringified nested dictionaries of configuration changes passed as part of CLI command. Used to modify the node configurations directly from the CLI. custom_nodes_parent_subdir (:obj:`str`): Relative path to parent folder which contains custom nodes that users have created to be used with PeekingDuck. For more information on using custom nodes, please refer to `Getting Started <getting_started/03_custom_nodes.html>`_. \"\"\" def __init__ ( self , pipeline_path : Path , config_updates_cli : str , custom_nodes_parent_subdir : str , ) -> None : self . logger = logging . getLogger ( __name__ ) self . pkd_base_dir = Path ( __file__ ) . resolve () . parent self . config_loader = ConfigLoader ( self . pkd_base_dir ) self . node_list = self . _load_node_list ( pipeline_path ) self . config_updates_cli = ast . literal_eval ( config_updates_cli ) custom_nodes_name = self . _get_custom_name_from_node_list () if custom_nodes_name is not None : custom_nodes_dir = ( Path . cwd () / custom_nodes_parent_subdir / custom_nodes_name ) self . custom_config_loader = ConfigLoader ( custom_nodes_dir ) sys . path . append ( custom_nodes_parent_subdir ) self . custom_nodes_dir = custom_nodes_dir def _load_node_list ( self , pipeline_path : Path ) -> \"NodeList\" : \"\"\"Loads a list of nodes from pipeline_path.yml\"\"\" # dotw 2022-03-17: Temporary helper methods def deprecation_warning ( name : str , config : Union [ str , Dict [ str , Any ]] ) -> None : self . logger . warning ( f \"` { name } ` deprecated, replaced by `input.visual`\" ) self . logger . warning ( f \"convert ` { name } ` to `input.visual`: { config } \" ) with open ( pipeline_path ) as node_yml : data = yaml . safe_load ( node_yml ) if not isinstance ( data , dict ) or \"nodes\" not in data : raise ValueError ( f \" { pipeline_path } has an invalid structure. \" \"Missing top-level 'nodes' key.\" ) nodes = data [ \"nodes\" ] if nodes is None : raise ValueError ( f \" { pipeline_path } does not contain any nodes!\" ) upgraded_nodes = [] for node in nodes : if isinstance ( node , str ): if node in [ \"input.live\" , \"input.recorded\" ]: deprecation_warning ( node , \"input.visual\" ) if node == \"input.live\" : node = { \"input.visual\" : { \"source\" : 0 }} else : self . logger . error ( \"input.recorded with no parameters error!\" ) node = \"input.visual\" else : if \"input.live\" in node : node_config = node . pop ( \"input.live\" ) if \"input_source\" in node_config : node_config [ \"source\" ] = node_config . pop ( \"input_source\" ) node [ \"input.visual\" ] = node_config deprecation_warning ( \"input.live\" , node_config ) if \"input.recorded\" in node : node_config = node . pop ( \"input.recorded\" ) if \"input_dir\" in node_config : node_config [ \"source\" ] = node_config . pop ( \"input_dir\" ) node [ \"input.visual\" ] = node_config deprecation_warning ( \"input.recorded\" , node_config ) upgraded_nodes . append ( node ) self . logger . info ( \"Successfully loaded pipeline file.\" ) return NodeList ( upgraded_nodes ) def _get_custom_name_from_node_list ( self ) -> Any : custom_name = None for node_str , _ in self . node_list : node_type = node_str . split ( \".\" )[ 0 ] if node_type not in PEEKINGDUCK_NODE_TYPES : custom_name = node_type break return custom_name def _instantiate_nodes ( self ) -> List [ AbstractNode ]: \"\"\"Given a list of imported nodes, instantiate nodes\"\"\" instantiated_nodes = [] for node_str , config_updates_yml in self . node_list : node_str_split = node_str . split ( \".\" ) self . logger . info ( f \"Initializing { node_str } node...\" ) if len ( node_str_split ) == 3 : # convert windows/linux filepath to a module path path_to_node = f \" { self . custom_nodes_dir . name } .\" node_name = \".\" . join ( node_str_split [ - 2 :]) instantiated_node = self . _init_node ( path_to_node , node_name , self . custom_config_loader , config_updates_yml , ) else : path_to_node = \"peekingduck.pipeline.nodes.\" instantiated_node = self . _init_node ( path_to_node , node_str , self . config_loader , config_updates_yml , ) instantiated_nodes . append ( instantiated_node ) return instantiated_nodes def _init_node ( self , path_to_node : str , node_name : str , config_loader : ConfigLoader , config_updates_yml : Optional [ Dict [ str , Any ]], ) -> AbstractNode : \"\"\"Imports node to filepath and initializes node with config.\"\"\" node = importlib . import_module ( path_to_node + node_name ) config = config_loader . get ( node_name ) # First, override default configs with values from pipeline_config.yml if config_updates_yml is not None : config = self . _edit_config ( config , config_updates_yml , node_name ) # Second, override configs again with values from cli if self . config_updates_cli is not None : if node_name in self . config_updates_cli . keys (): config = self . _edit_config ( config , self . config_updates_cli [ node_name ], node_name ) return node . Node ( config ) def _edit_config ( self , dict_orig : Dict [ str , Any ], dict_update : Dict [ str , Any ], node_name : str , ) -> Dict [ str , Any ]: \"\"\"Update value of a nested dictionary of varying depth using recursion\"\"\" for key , value in dict_update . items (): if isinstance ( value , collections . abc . Mapping ): dict_orig [ key ] = self . _edit_config ( dict_orig . get ( key , {}), value , node_name # type: ignore ) else : if key not in dict_orig : self . logger . warning ( f \"Config for node { node_name } does not have the key: { key } \" ) else : if key == \"detect_ids\" : key , value = obj_det_change_class_name_to_id ( node_name , key , value ) dict_orig [ key ] = value self . logger . info ( f \"Config for node { node_name } is updated to: ' { key } ': { value } \" ) return dict_orig def get_pipeline ( self ) -> Pipeline : \"\"\"Returns a compiled :py:class:`Pipeline <peekingduck.pipeline.pipeline.Pipeline>` for PeekingDuck :py:class:`Runner <peekingduck.runner.Runner>` to execute. \"\"\" instantiated_nodes = self . _instantiate_nodes () try : return Pipeline ( instantiated_nodes ) except ValueError as error : self . logger . error ( str ( error )) sys . exit ( 1 ) class NodeList : \"\"\"Iterator class to return node string and node configs (if any) from the nodes declared in the run config file. \"\"\" def __init__ ( self , nodes : List [ Union [ Dict [ str , Any ], str ]]) -> None : self . nodes = nodes self . length = len ( nodes ) def __iter__ ( self ) -> Iterator [ Tuple [ str , Optional [ Dict [ str , Any ]]]]: self . current = - 1 return self def __next__ ( self ) -> Tuple [ str , Optional [ Dict [ str , Any ]]]: self . current += 1 if self . current >= self . length : raise StopIteration node_item = self . nodes [ self . current ] if isinstance ( node_item , dict ): node_str = next ( iter ( node_item )) config_updates = node_item [ node_str ] else : node_str = node_item config_updates = None return node_str , config_updates","title":"Source Code Tracing"},{"location":"pkd/#the-abstract-node-class","text":"abstract node class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from typing import Any , Dict from peekingduck.pipeline.nodes.node import AbstractNode class Node ( AbstractNode ): \"\"\"This is a template class of how to write a node for PeekingDuck. Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) # initialize/load any configs and models here # configs can be called by self.<config_name> e.g. self.filepath # self.logger.info(f\"model loaded with configs: config\") def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node does ___. Args: inputs (dict): Dictionary with keys \"__\", \"__\". Returns: outputs (dict): Dictionary with keys \"__\". \"\"\" # result = do_something(inputs[\"in1\"], inputs[\"in2\"]) # outputs = {\"out1\": result} # return outputs","title":"The Abstract Node Class"},{"location":"pkd/#abc-class","text":"Notice that all custom nodes are derived from the AbstractNode class from peekingduck.pipeline.nodes.node import AbstractNode . This class is the base class for all custom nodes, as we will soon see. @abstractmethod : This is a decorator which indicates that the method is abstract. More concretely, once a class inherits from AbstractNode , it must implement all the abstract methods. In this case, we need to implement the run method in each custom node. abstract method 1 2 3 4 @abstractmethod def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"abstract method needed for running node\"\"\" raise NotImplementedError ( \"This method needs to be implemented\" )","title":"ABC Class"},{"location":"pkd/#the-run-method","text":"As we previously see, the custom nodes are derived from the AbstractNode class. Each custom node must implement the run method. A brief check tells me that the output of the run method is a dictionary of strings and values. This is reasonable since we can see from line 133 in runner.py that outputs = node.run(inputs) , and outputs is indeed a dictionary of strings and values. We can also find out what keys the output hold by going to visual.yml and looking at the output key. The logic holds for other default nodes.","title":"The Run Method"},{"location":"pkd/#problems","text":"","title":"Problems"},{"location":"pkd/#chaining-yolo-and-posenet","text":"I set all thresholds from Yolo and Posenet to be \\(0\\) to avoid any quality check on the bboxes so that I can attempt to debug the issue. The issue turns out to be pretty confusing, when inputs were printed in the dabble.exercise_counter , it turns out that some bboxes were populated but the bbox_scores were not. This should not be the case since a bijective relationship should be formed. Upon further digging, here is what I found: Using yolov4tiny , the model did not manage to predict the person class. Thus outputs dict consisting of bboxes, bbox_labels, bbox_scores were empty in the model.yolo portion. We turn our attention to model.posenet , it turns out that bboxes were again predicted at the Posenet level. This tells us half the story, because bboxes variable is now indeed populated (if Posenet says yes) but bbox_scores variable is still empty. bboxes , keypoints , keypoint_scores , keypoint_conns = self . model . predict ( inputs [ \"img\" ] ) bbox_labels = np . array ([ \"person\" ] * len ( bboxes )) bboxes = np . clip ( bboxes , 0 , 1 ) outputs = { \"bboxes\" : bboxes , \"keypoints\" : keypoints , \"keypoint_scores\" : keypoint_scores , \"keypoint_conns\" : keypoint_conns , \"bbox_labels\" : bbox_labels , } Consequently, when unpacking in the dabble.exercise_counter.py , img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] The bboxes are not empty but bbox_scores is, resulting in error. A bit more digging into why the bboxes are returned from Posenet. We take a look into self.model = posenet_model.PoseNetModel(self.config) which points to model.posenetv1.posenet_model , and the predict method points to Predictor class in model.posenetv1.postnet_files.predictor . It turns out the bbox is derived from bbox = self._get_bbox_of_one_pose(pose_coords, pose_masks) and these are actually the bounding boxes of the keypoints . So if there are 17 pairs of keypoints, we simply find the max corners of them. (i.e. imagine the keypoints depicts a human skeleton and we enclose them with bboxes). Something worth noting is in model.posenet , bbox_labels = np.array([\"person\"] * len(bboxes)) was coded to depict person . Need to add a clause to catch when yolo does not predict anything handling empty bbox_scores . I believe the purpose of chaining yolo then posenet is to let yolo get the person's bounding box coordinates first, then chain the cropped person to posenet for keypoint predictions... KIV first. Created debug file for yolo , posenet and exercise_counter as v4tiny seem to return errors on index out of range. Turns out yolov4tiny is not capable of detecting human sometimes, creating empty bboxes and scores, but get overwritten by bboxes found in posenet , which is a bit different from the bboxes in yolo. In the pipeline where we chain posenet after yolo , I created another debug file before exercise_counter and confirmed that if both yolo and posenet have bbox values, the latter posenet will overwrite the one from yolo . This setting is nodes : - input . visual : source : https : // storage . googleapis . com / reighns / peekingduck / videos / push_ups . mp4 - model . yolo : model_type : \"v4\" # \"v4tiny\" iou_threshold : 0.1 score_threshold : 0.1 detect_ids : [ \"person\" ] # [0] num_classes : 1 - custom_nodes . dabble . debug_yolo - model . posenet : model_type : \"resnet\" resolution : { height : 224 , width : 224 } score_threshold : 0.05 - custom_nodes . dabble . debug_posenet - dabble . fps - custom_nodes . dabble . debug_exercise_counter - custom_nodes . dabble . exercise_counter - draw . poses # - model.mtcnn # - draw.mosaic_bbox - draw . legend : show : [ \"fps\" ] - output . screen","title":"Chaining Yolo and Posenet"},{"location":"workflows/","text":"PeekingDuck Workflow by Hongnan Gao 1st May, 2022 This section details some workflow tools that I used for this project. Setup Main Directory (IDE) Let us create our main directory for the project: creating main directory 1 2 3 $ mkdir pkd_exercise_counter $ cd pkd_exercise_counter $ code . # (1) Open the project directory in Visual Studio Code. To change appropriately if using different IDE. Virtual Environment Set up a virtual environment in your IDE. Virtual Environment If you are using Linux or Mac, then you may need to install the virtual environment manager. For windows, python comes with a virtual environment manager venv installed. install venv 1 2 $ sudo apt install python3.8 python3.8-venv python3-venv # For Ubuntu $ pip3 install virtualenv # For Mac You can activate the virtual environment (assuming Windows) as follows: virtual environment windows 1 2 3 $ python -m venv venv_pkd_exercise_counter # (1) $ . \\v env_pkd_exercise_counter \\S cripts \\a ctivate # (2) ( venv ) $ python -m pip install --upgrade pip setuptools wheel # (3) Create virtual environment. Activate virtual environment. Upgrade pip. Note Although the virtual environment name is venv_pkd_exercise_counter , it is too long and I will use venv for future references. You should see the following directory structure: main directory tree 1 2 pkd_exercise_counter/ \u2514\u2500\u2500 venv_pkd_exercise_counter/ Requirements and Setup Note We note that echo > \"filename\" command is used to create a file in Windows. One can use touch in other OS such as macOS or even code if you are using Visual Studio Code. creating requirements 1 2 3 ( venv ) $ echo > setup.py ( venv ) $ echo > requirements.txt ( venv ) $ pip install -e . [ Line 1 -2 ] : setup.py file informs you about the module or package-dependencies you are about to install has been packaged and distributed with Distutils, which is the standard for distributing Python Modules. You can skip setup.py if you are just using requirements.txt to install dependencies. [ Line 3 ] : Installs packages from requirements.txt . One can also use commands such as python -m pip install -e \".[dev]\" to install additional dev packages specified in setup.py . After which we quickly run a verification to see if PeekingDuck is installed correctly. peekingduck verification 1 ( venv ) $ peekingduck --verify_install Info In my setup.py , I specified python to be \\(3.8\\) and above. This has been tested on ubuntu latest and windows latest in GitHub Actions. You should see the following directory structure: main directory tree 1 2 3 4 pkd_exercise_counter/ \u251c\u2500\u2500 venv_pkd_exercise_counter/ \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 setup.py Git Git is a version control system that is used to track changes to files. It is integral to the development process of any software. Here we initiate our main directory with git. Note The commands below may differ depending on personal style and preferences. (i.e. ssh or https) git 1 2 3 4 5 6 7 8 9 10 ( venv ) $ echo > README.md ( venv ) $ echo > .gitignore ( venv ) $ git init ( venv ) $ git config --global user.name \"Your Name\" ( venv ) $ git config --global user.email \"your@email.com\" # (1) ( venv ) $ git add . ( venv ) $ git commit -a # (2) ( venv ) $ git remote add origin \"your-repo-http\" # (3) ( venv ) $ git remote set-url origin https:// [ token ] @github.com/ [ username ] / [ repository ] # (4) ( venv ) $ git push origin master -u # (5) important to set the email linked to the git account. write commit message. add remote origin. set the remote origin. push to remote origin. Styling and Formatting We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options) 1 . black : an in-place reformatter that (mostly) adheres to PEP8. isort : sorts and formats import statements inside Python scripts. flake8 : a code linter with stylistic conventions that adhere to PEP8. We also have pyproject.toml and .flake8 to configure our formatter and linter. create pyproject.toml and .flake8 1 2 ( venv ) $ echo > pyproject.toml ( venv ) $ echo > .flake8 For example, the configuration for black below tells us that our maximum line length should be \\(79\\) characters. We also want to exclude certain file extensions and in particular the virtual environment folder we created earlier. pyproject.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Black formatting [tool.black] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_* )/ ''' You can run black --check to check if your code is formatted correctly or black . to format your code. Mkdocs Mkdocs Setup We will be using Mkdocs to generate our markdown documentation into a static website. The following requirements are necessary to run mkdocs : requirements.txt 1 2 3 4 mkdocs 1.3.0 mkdocs-material 8.2.13 mkdocs-material-extensions 1.0.3 mkdocstrings 0.18.1 Initialize default template by calling mkdocs new . where . refers to the current directory. The . can be replaced with a path to your directory as well. Subsequently, a folder docs alongside with mkdocs.yml file will be created. mkdocs folder structure 1 2 3 4 5 6 7 pkd_exercise_counter/ \u251c\u2500\u2500 venv_pkd_exercise_counter/ \u251c\u2500\u2500 docs/ \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 setup.py We can specify the following configurations in mkdocs.yml : Show/Hide mkdocs.yml mkdocs.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 site_name: Hongnan G. PeekingDuck Exercise Counter site_url: \"\" nav: - Home: index.md - PeekingDuck: - Setup: workflows.md - Push-up Counter: pushup.md theme: name: material features: - content.code.annotate markdown_extensions: - attr_list - md_in_html - admonition - footnotes - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.superfences - pymdownx.snippets - pymdownx.details - pymdownx.arithmatex: generic: true extra_javascript: - javascript/mathjax.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js extra_css: - css/extra.css plugins: - search - mkdocstrings # plugins for mkdocstrings Some of the key features include: Code block Line Numbering ; Code block Annotations ; MathJax . One missing feature is the ability to toggle code blocks. Two workarounds are provided: Toggle Using Admonition Setting Up mkdir custom_hn_push_up_counter Toggle Using HTML Setting Up mkdir custom_hn_push_up_counter We added some custom CSS and JavaScript files. In particular, we added mathjax.js for easier latex integration. You can now call mkdocs serve to start the server at a local host to view your document. Tip To link to a section or header, you can do this: [link to Styling and Formatting by workflows.md#styling-and-formatting . Mkdocstrings We also can create docstrings as API reference using Mkdocstrings : Install mkdocstrings: pip install mkdocstrings Place plugings to mkdocs.yml : mkdocs.yml 1 2 3 plugins: - search - mkdocstrings In mkdocs.yml 's navigation tree: mkdocs.yml 1 2 - API Documentation: - Exercise Counter: api/exercise_counter_api.md For example you have a python file called exercise_counter.py and want to render it, create a file named api/exercise_counter_api.md and in this markdown file: api/exercise_counter_api.md 1 ::: custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter # package path. Tests Set up pytest for testing codes. Install pytest 1 2 pytest == 6 .0.2 pytest-cov == 2 .10.1 In general, Pytest expects our testing codes to be grouped under a folder called tests . We can configure in our pyproject.toml file to override this if we wish to ask pytest to check from a different directory. After specifying the folder holding the test codes, pytest will then look for python scripts starting with tests_*.py ; we can also change the extensions accordingly if you want pytest to look for other kinds of files (extensions) 2 . pyproject.toml 1 2 3 4 # Pytest [ tool.pytest.ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\" CI/CD (GitHub Actions) The following content is with reference to: MLOps Basics [Week 6]: CI/CD - GitHub Actions CI/CD for Machine Learning We will be using GitHub Actions to setup our mini CI/CD. Commit Checks Commit checks is to ensure the following: The requirements can be installed on various OS and python versions. Ensure code quality and adherence to PEP8 (or other coding standards). Ensure tests are passed. lint_test.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 name : Commit Checks # (1) on : [ push , pull_request ] # (2) jobs : # (3) check_code : # (4) runs-on : ${{ matrix.os }} # (5) strategy : # (6) fail-fast : false # (7) matrix : # (8) os : [ ubuntu-latest , windows-latest ] # (9) python-version : [ 3.8 , 3.9 ] # (10) steps : # (11) - name : Checkout code # (12) uses : actions/checkout@v2 # (13) - name : Setup Python # (14) uses : actions/setup-python@v2 # (15) with : # (16) python-version : ${{ matrix.python-version }} # (17) cache : \"pip\" # (18) - name : Install dependencies # (19) run : | # (20) python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Run Black Formatter # (21) run : black --check . # (22) # - name: Run flake8 Linter # run: flake8 . # look at my pyproject.toml file and see if there is a flake8 section, if so, run flake8 on the files in the flake8 section - name : Run Pytest # (23) run : python -m coverage run --source=custom_hn_exercise_counter -m pytest && python -m coverage report # (24) This is the name that will show up under the Actions tab in GitHub. Typically, we should name it appropriately like how we indicate the subject of an email. The list here indicates the workflow will be triggered whenever someone directly pushes or submits a PR to the main branch. Once an event is triggered, a set of jobs will run on a runner . In our example, we will run a job called check_code on a runner to check for formatting and linting errors as well as run the pytest tests. This is the name of the job that will run on the runner. We specify which OS system we want the code to be run on. We can simply say ubuntu-latest or windows-latest if we just want the code to be tested on a single OS. However, here we want to check if it works on both Ubuntu and Windows, and hence we define ${{ matrix.os }} where matrix.os is [ubuntu-latest, windows-latest] . A cartesian product is created for us and the job will run on both OSs. Strategy is a way to control how the jobs are run. In our example, we want the job to run as fast as possible, so we set strategy.fail-fast to false . If one job fails, then the whole workflow will fail, this is not ideal if we want to test multiple jobs, we can set fail-fast to false to allow the workflow to continue running on the remaining jobs. Matrix is a way to control how the jobs are run. In our example, we want to run the job on both Python 3.8 and 3.9, so we set matrix.python-version to [3.8, 3.9] . This list consists of the OS that the job will run on in cartesian product. This is the python version that the job will run on in cartesian product. We can simply say 3.8 or 3.9 if we just want the code to be tested on a single python version. However, here we want to check if it works on both python 3.8 and python 3.9, and hence we define ${{ matrix.python-version }} where matrix.python-version is [3.8, 3.9] . A cartesian product is created for us and the job will run on both python versions. This is a list of dictionaries that defines the steps that will be run. Name is the name of the step that will be run. It is important to specify @v2 as if unspecified, then the workflow will use the latest version from actions/checkout template, potentially causing libraries to break. The idea here is like your requirements.txt idea, if different versions then will break. Setup Python is a step that will be run before the job. Same as above, we specify @v2 as if unspecified, then the workflow will use the latest version from actions/setup-python template, potentially causing libraries to break. With is a way to pass parameters to the step. This is the python version that the job will run on in cartesian product and if run 1 python version then can define as just say 3.7 Cache is a way to control how the libraries are installed. Install dependencies is a step that will be run before the job. | is multi-line string that runs the below code, which sets up the libraries from setup.py file. Run Black Formatter is a step that will be run before the job. Runs black with configurations from pyproject.toml file. Run Pytest is a step that will be run before the job. Runs pytest, note that I specified python -m to resolve PATH issues. Deploy to Website The other workflow for this project is to deploy the website built from Mkdocsto gh-pages branch. Show/Hide content for deploy_website.yml deploy_website.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 name : Deploy Website to GitHub Pages on : push : branches : [ master ] paths : - \"docs/**\" - \"mkdocs.yml\" - \".github/workflows/deploy_website.yml\" permissions : write-all jobs : deploy : runs-on : ubuntu-latest name : Deploy Website steps : - uses : actions/checkout@v2 - name : Set Up Python uses : actions/setup-python@v2 with : python-version : 3.8 architecture : x64 - name : Install dependencies run : | # this symbol is called a multiline string python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Build Website run : | mkdocs build - name : Push Built Website to gh-pages Branch run : | git config --global user.name 'Hongnan G.' git config --global user.email 'reighns92@users.noreply.github.com' ghp-import \\ --no-jekyll \\ --force \\ --no-history \\ --push \\ --message \"Deploying ${{ github.sha }}\" \\ site This part is extracted from madewithml . \u21a9 This part is extracted from madewithml . \u21a9","title":"Workflows"},{"location":"workflows/#setup-main-directory-ide","text":"Let us create our main directory for the project: creating main directory 1 2 3 $ mkdir pkd_exercise_counter $ cd pkd_exercise_counter $ code . # (1) Open the project directory in Visual Studio Code. To change appropriately if using different IDE.","title":"Setup Main Directory (IDE)"},{"location":"workflows/#virtual-environment","text":"Set up a virtual environment in your IDE. Virtual Environment If you are using Linux or Mac, then you may need to install the virtual environment manager. For windows, python comes with a virtual environment manager venv installed. install venv 1 2 $ sudo apt install python3.8 python3.8-venv python3-venv # For Ubuntu $ pip3 install virtualenv # For Mac You can activate the virtual environment (assuming Windows) as follows: virtual environment windows 1 2 3 $ python -m venv venv_pkd_exercise_counter # (1) $ . \\v env_pkd_exercise_counter \\S cripts \\a ctivate # (2) ( venv ) $ python -m pip install --upgrade pip setuptools wheel # (3) Create virtual environment. Activate virtual environment. Upgrade pip. Note Although the virtual environment name is venv_pkd_exercise_counter , it is too long and I will use venv for future references. You should see the following directory structure: main directory tree 1 2 pkd_exercise_counter/ \u2514\u2500\u2500 venv_pkd_exercise_counter/","title":"Virtual Environment"},{"location":"workflows/#requirements-and-setup","text":"Note We note that echo > \"filename\" command is used to create a file in Windows. One can use touch in other OS such as macOS or even code if you are using Visual Studio Code. creating requirements 1 2 3 ( venv ) $ echo > setup.py ( venv ) $ echo > requirements.txt ( venv ) $ pip install -e . [ Line 1 -2 ] : setup.py file informs you about the module or package-dependencies you are about to install has been packaged and distributed with Distutils, which is the standard for distributing Python Modules. You can skip setup.py if you are just using requirements.txt to install dependencies. [ Line 3 ] : Installs packages from requirements.txt . One can also use commands such as python -m pip install -e \".[dev]\" to install additional dev packages specified in setup.py . After which we quickly run a verification to see if PeekingDuck is installed correctly. peekingduck verification 1 ( venv ) $ peekingduck --verify_install Info In my setup.py , I specified python to be \\(3.8\\) and above. This has been tested on ubuntu latest and windows latest in GitHub Actions. You should see the following directory structure: main directory tree 1 2 3 4 pkd_exercise_counter/ \u251c\u2500\u2500 venv_pkd_exercise_counter/ \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 setup.py","title":"Requirements and Setup"},{"location":"workflows/#git","text":"Git is a version control system that is used to track changes to files. It is integral to the development process of any software. Here we initiate our main directory with git. Note The commands below may differ depending on personal style and preferences. (i.e. ssh or https) git 1 2 3 4 5 6 7 8 9 10 ( venv ) $ echo > README.md ( venv ) $ echo > .gitignore ( venv ) $ git init ( venv ) $ git config --global user.name \"Your Name\" ( venv ) $ git config --global user.email \"your@email.com\" # (1) ( venv ) $ git add . ( venv ) $ git commit -a # (2) ( venv ) $ git remote add origin \"your-repo-http\" # (3) ( venv ) $ git remote set-url origin https:// [ token ] @github.com/ [ username ] / [ repository ] # (4) ( venv ) $ git push origin master -u # (5) important to set the email linked to the git account. write commit message. add remote origin. set the remote origin. push to remote origin.","title":"Git"},{"location":"workflows/#styling-and-formatting","text":"We will be using a very popular blend of style and formatting conventions that makes some very opinionated decisions on our behalf (with configurable options) 1 . black : an in-place reformatter that (mostly) adheres to PEP8. isort : sorts and formats import statements inside Python scripts. flake8 : a code linter with stylistic conventions that adhere to PEP8. We also have pyproject.toml and .flake8 to configure our formatter and linter. create pyproject.toml and .flake8 1 2 ( venv ) $ echo > pyproject.toml ( venv ) $ echo > .flake8 For example, the configuration for black below tells us that our maximum line length should be \\(79\\) characters. We also want to exclude certain file extensions and in particular the virtual environment folder we created earlier. pyproject.toml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Black formatting [tool.black] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.eggs # exclude a few common directories in the | \\.git # root of the project | \\.hg | \\.mypy_cache | \\.tox | _build | buck-out | build | dist | venv_* )/ ''' You can run black --check to check if your code is formatted correctly or black . to format your code.","title":"Styling and Formatting"},{"location":"workflows/#mkdocs","text":"","title":"Mkdocs"},{"location":"workflows/#mkdocs-setup","text":"We will be using Mkdocs to generate our markdown documentation into a static website. The following requirements are necessary to run mkdocs : requirements.txt 1 2 3 4 mkdocs 1.3.0 mkdocs-material 8.2.13 mkdocs-material-extensions 1.0.3 mkdocstrings 0.18.1 Initialize default template by calling mkdocs new . where . refers to the current directory. The . can be replaced with a path to your directory as well. Subsequently, a folder docs alongside with mkdocs.yml file will be created. mkdocs folder structure 1 2 3 4 5 6 7 pkd_exercise_counter/ \u251c\u2500\u2500 venv_pkd_exercise_counter/ \u251c\u2500\u2500 docs/ \u2502 \u2514\u2500\u2500 index.md \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 requirements.txt \u2514\u2500\u2500 setup.py We can specify the following configurations in mkdocs.yml : Show/Hide mkdocs.yml mkdocs.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 site_name: Hongnan G. PeekingDuck Exercise Counter site_url: \"\" nav: - Home: index.md - PeekingDuck: - Setup: workflows.md - Push-up Counter: pushup.md theme: name: material features: - content.code.annotate markdown_extensions: - attr_list - md_in_html - admonition - footnotes - pymdownx.highlight - pymdownx.inlinehilite - pymdownx.superfences - pymdownx.snippets - pymdownx.details - pymdownx.arithmatex: generic: true extra_javascript: - javascript/mathjax.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js extra_css: - css/extra.css plugins: - search - mkdocstrings # plugins for mkdocstrings Some of the key features include: Code block Line Numbering ; Code block Annotations ; MathJax . One missing feature is the ability to toggle code blocks. Two workarounds are provided: Toggle Using Admonition Setting Up mkdir custom_hn_push_up_counter Toggle Using HTML Setting Up mkdir custom_hn_push_up_counter We added some custom CSS and JavaScript files. In particular, we added mathjax.js for easier latex integration. You can now call mkdocs serve to start the server at a local host to view your document. Tip To link to a section or header, you can do this: [link to Styling and Formatting by workflows.md#styling-and-formatting .","title":"Mkdocs Setup"},{"location":"workflows/#mkdocstrings","text":"We also can create docstrings as API reference using Mkdocstrings : Install mkdocstrings: pip install mkdocstrings Place plugings to mkdocs.yml : mkdocs.yml 1 2 3 plugins: - search - mkdocstrings In mkdocs.yml 's navigation tree: mkdocs.yml 1 2 - API Documentation: - Exercise Counter: api/exercise_counter_api.md For example you have a python file called exercise_counter.py and want to render it, create a file named api/exercise_counter_api.md and in this markdown file: api/exercise_counter_api.md 1 ::: custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter # package path.","title":"Mkdocstrings"},{"location":"workflows/#tests","text":"Set up pytest for testing codes. Install pytest 1 2 pytest == 6 .0.2 pytest-cov == 2 .10.1 In general, Pytest expects our testing codes to be grouped under a folder called tests . We can configure in our pyproject.toml file to override this if we wish to ask pytest to check from a different directory. After specifying the folder holding the test codes, pytest will then look for python scripts starting with tests_*.py ; we can also change the extensions accordingly if you want pytest to look for other kinds of files (extensions) 2 . pyproject.toml 1 2 3 4 # Pytest [ tool.pytest.ini_options ] testpaths = [ \"tests\" ] python_files = \"test_*.py\"","title":"Tests"},{"location":"workflows/#cicd-github-actions","text":"The following content is with reference to: MLOps Basics [Week 6]: CI/CD - GitHub Actions CI/CD for Machine Learning We will be using GitHub Actions to setup our mini CI/CD.","title":"CI/CD (GitHub Actions)"},{"location":"workflows/#commit-checks","text":"Commit checks is to ensure the following: The requirements can be installed on various OS and python versions. Ensure code quality and adherence to PEP8 (or other coding standards). Ensure tests are passed. lint_test.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 name : Commit Checks # (1) on : [ push , pull_request ] # (2) jobs : # (3) check_code : # (4) runs-on : ${{ matrix.os }} # (5) strategy : # (6) fail-fast : false # (7) matrix : # (8) os : [ ubuntu-latest , windows-latest ] # (9) python-version : [ 3.8 , 3.9 ] # (10) steps : # (11) - name : Checkout code # (12) uses : actions/checkout@v2 # (13) - name : Setup Python # (14) uses : actions/setup-python@v2 # (15) with : # (16) python-version : ${{ matrix.python-version }} # (17) cache : \"pip\" # (18) - name : Install dependencies # (19) run : | # (20) python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Run Black Formatter # (21) run : black --check . # (22) # - name: Run flake8 Linter # run: flake8 . # look at my pyproject.toml file and see if there is a flake8 section, if so, run flake8 on the files in the flake8 section - name : Run Pytest # (23) run : python -m coverage run --source=custom_hn_exercise_counter -m pytest && python -m coverage report # (24) This is the name that will show up under the Actions tab in GitHub. Typically, we should name it appropriately like how we indicate the subject of an email. The list here indicates the workflow will be triggered whenever someone directly pushes or submits a PR to the main branch. Once an event is triggered, a set of jobs will run on a runner . In our example, we will run a job called check_code on a runner to check for formatting and linting errors as well as run the pytest tests. This is the name of the job that will run on the runner. We specify which OS system we want the code to be run on. We can simply say ubuntu-latest or windows-latest if we just want the code to be tested on a single OS. However, here we want to check if it works on both Ubuntu and Windows, and hence we define ${{ matrix.os }} where matrix.os is [ubuntu-latest, windows-latest] . A cartesian product is created for us and the job will run on both OSs. Strategy is a way to control how the jobs are run. In our example, we want the job to run as fast as possible, so we set strategy.fail-fast to false . If one job fails, then the whole workflow will fail, this is not ideal if we want to test multiple jobs, we can set fail-fast to false to allow the workflow to continue running on the remaining jobs. Matrix is a way to control how the jobs are run. In our example, we want to run the job on both Python 3.8 and 3.9, so we set matrix.python-version to [3.8, 3.9] . This list consists of the OS that the job will run on in cartesian product. This is the python version that the job will run on in cartesian product. We can simply say 3.8 or 3.9 if we just want the code to be tested on a single python version. However, here we want to check if it works on both python 3.8 and python 3.9, and hence we define ${{ matrix.python-version }} where matrix.python-version is [3.8, 3.9] . A cartesian product is created for us and the job will run on both python versions. This is a list of dictionaries that defines the steps that will be run. Name is the name of the step that will be run. It is important to specify @v2 as if unspecified, then the workflow will use the latest version from actions/checkout template, potentially causing libraries to break. The idea here is like your requirements.txt idea, if different versions then will break. Setup Python is a step that will be run before the job. Same as above, we specify @v2 as if unspecified, then the workflow will use the latest version from actions/setup-python template, potentially causing libraries to break. With is a way to pass parameters to the step. This is the python version that the job will run on in cartesian product and if run 1 python version then can define as just say 3.7 Cache is a way to control how the libraries are installed. Install dependencies is a step that will be run before the job. | is multi-line string that runs the below code, which sets up the libraries from setup.py file. Run Black Formatter is a step that will be run before the job. Runs black with configurations from pyproject.toml file. Run Pytest is a step that will be run before the job. Runs pytest, note that I specified python -m to resolve PATH issues.","title":"Commit Checks"},{"location":"workflows/#deploy-to-website","text":"The other workflow for this project is to deploy the website built from Mkdocsto gh-pages branch. Show/Hide content for deploy_website.yml deploy_website.yml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 name : Deploy Website to GitHub Pages on : push : branches : [ master ] paths : - \"docs/**\" - \"mkdocs.yml\" - \".github/workflows/deploy_website.yml\" permissions : write-all jobs : deploy : runs-on : ubuntu-latest name : Deploy Website steps : - uses : actions/checkout@v2 - name : Set Up Python uses : actions/setup-python@v2 with : python-version : 3.8 architecture : x64 - name : Install dependencies run : | # this symbol is called a multiline string python -m pip install --upgrade pip setuptools wheel pip install -e . - name : Build Website run : | mkdocs build - name : Push Built Website to gh-pages Branch run : | git config --global user.name 'Hongnan G.' git config --global user.email 'reighns92@users.noreply.github.com' ghp-import \\ --no-jekyll \\ --force \\ --no-history \\ --push \\ --message \"Deploying ${{ github.sha }}\" \\ site This part is extracted from madewithml . \u21a9 This part is extracted from madewithml . \u21a9","title":"Deploy to Website"},{"location":"api/exercise_counter_api/","text":"Custom node to show keypoints and count the number of push ups (sit ups). GlobalParams dataclass Global parameters for the node. Attributes: Name Type Description FONT int Font for the text. WHITE int White color in BGR. YELLOW int Yellow color in BGR. KP_NAME_TO_INDEX Dict[str, int] PoseNet/MoveNet's skeletal keypoints name to index mapping. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py class GlobalParams : \"\"\"Global parameters for the node. Attributes: FONT (int): Font for the text. WHITE (int): White color in BGR. YELLOW (int): Yellow color in BGR. KP_NAME_TO_INDEX (Dict[str, int]): PoseNet/MoveNet's skeletal keypoints name to index mapping. \"\"\" FONT : int = cv2 . FONT_HERSHEY_SIMPLEX WHITE : Tuple [ int ] = ( 255 , 255 , 255 ) YELLOW : Tuple [ int ] = ( 0 , 255 , 255 ) KP_NAME_TO_INDEX : Dict [ str , int ] = field ( default_factory = lambda : { \"nose\" : 0 , \"left_eye\" : 1 , \"right_eye\" : 2 , \"left_ear\" : 3 , \"right_ear\" : 4 , \"left_shoulder\" : 5 , \"right_shoulder\" : 6 , \"left_elbow\" : 7 , \"right_elbow\" : 8 , \"left_wrist\" : 9 , \"right_wrist\" : 10 , \"left_hip\" : 11 , \"right_hip\" : 12 , \"left_knee\" : 13 , \"right_knee\" : 14 , \"left_ankle\" : 15 , \"right_ankle\" : 16 , } ) Node ( AbstractNode ) Custom node to display keypoints and count number of hand waves Parameters: Name Type Description Default config obj: Dict[str, Any] | :obj: None ): Node configuration. None config.exercise_name str Name of the exercise. Default: \"pushups\". required config.keypoint_threshold float Ignore keypoints below this threshold. Default: 0.3. required config.push_up_pose_params obj: Dict[str, Any] ): Parameters for the push up pose. Default: {starting_elbow_angle: 155, ending_elbow_angle: 90}. required Attributes: Name Type Description self.frame_count int Track the number of frames processed. self.expected_pose str The expected pose. Default: \"down\". self.num_push_ups float Cumulative number of push ups. Default: 0. self.have_started_push_ups bool Whether or not the push ups have started. Default: False. self.elbow_angle float Angle of the elbow. Default: None. self.global_params_dataclass GlobalParams Global parameters for the node. self.push_up_pose_params_dataclass PushupPoseParams Push up pose parameters. self.interested_keypoints List[str] List of keypoints to track. Default: [\"left_elbow\", \"left_shoulder\", \"left_wrist\"]. self.left_elbow float Keypoints of the left elbow. Default: None. self.left_shoulder float Keypoints of the left shoulder. Default: None. self.left_wrist float Keypoints of the left wrist. Default: None. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py class Node ( AbstractNode ): \"\"\"Custom node to display keypoints and count number of hand waves Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. config.exercise_name (str): Name of the exercise. Default: \"pushups\". config.keypoint_threshold (float): Ignore keypoints below this threshold. Default: 0.3. config.push_up_pose_params (:obj:`Dict[str, Any]`): Parameters for the push up pose. Default: {starting_elbow_angle: 155, ending_elbow_angle: 90}. Attributes: self.frame_count (int): Track the number of frames processed. self.expected_pose (str): The expected pose. Default: \"down\". self.num_push_ups (float): Cumulative number of push ups. Default: 0. self.have_started_push_ups (bool): Whether or not the push ups have started. Default: False. self.elbow_angle (float): Angle of the elbow. Default: None. self.global_params_dataclass (GlobalParams): Global parameters for the node. self.push_up_pose_params_dataclass (PushupPoseParams): Push up pose parameters. self.interested_keypoints (List[str]): List of keypoints to track. Default: [\"left_elbow\", \"left_shoulder\", \"left_wrist\"]. self.left_elbow (float): Keypoints of the left elbow. Default: None. self.left_shoulder (float): Keypoints of the left shoulder. Default: None. self.left_wrist (float): Keypoints of the left wrist. Default: None. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) self . logger = logging . getLogger ( __name__ ) self . exercise_name : str self . keypoint_threshold : float # ignore keypoints below this threshold self . push_up_pose_params : Dict [ str , Any ] self . logger . info ( f \"Initialize Exercise Type: { self . exercise_name } !\" ) self . frame_count = 0 self . expected_pose = \"down\" self . num_push_ups = 0 self . have_started_push_ups = False self . elbow_angle = None self . global_params_dataclass = GlobalParams () self . push_up_pose_params_dataclass = PushupPoseParams . from_dict ( self . push_up_pose_params ) self . interested_keypoints = [ \"left_elbow\" , \"left_shoulder\" , \"left_wrist\" , ] # each element in the self.interested_keypoints list will now become an attribute initialized to None self . reset_keypoints_to_none () def reset_keypoints_to_none ( self ) -> None : \"\"\"Reset all keypoints attributes to None after each frame.\"\"\" for interested_keypoint in self . interested_keypoints : setattr ( self , interested_keypoint , None ) def inc_num_push_ups ( self ) -> float : \"\"\"Increments the number of push ups by 0.5 for every directional change. Returns: self.num_push_ups (float): Cumulative number of push ups. \"\"\" self . num_push_ups += 0.5 return self . num_push_ups def is_up_pose ( self , elbow_angle : float ) -> bool : \"\"\"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a up pose, False otherwise. \"\"\" return ( elbow_angle > self . push_up_pose_params_dataclass . starting_elbow_angle ) def is_down_pose ( self , elbow_angle : float ) -> None : \"\"\"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a down pose, False otherwise. \"\"\" return ( elbow_angle <= self . push_up_pose_params_dataclass . ending_elbow_angle ) @staticmethod def is_bbox_or_keypoints_empty ( bboxes : np . ndarray , keypoints : np . ndarray , keypoint_scores : np . ndarray , ) -> bool : \"\"\"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Args: bboxes (np.ndarray): The bounding boxes. keypoints (np.ndarray): The keypoints. keypoint_scores (np.ndarray): The keypoint scores. Returns: bool: True if the bounding box or keypoints are empty, False otherwise. \"\"\" return ( len ( bboxes ) == 0 or len ( keypoints ) == 0 or len ( keypoint_scores ) == 0 ) # pylint: disable=too-many-locals def count_push_ups ( self , img : np . ndarray , img_size : Tuple [ int , int ], the_keypoints : np . ndarray , the_keypoint_scores : np . ndarray , ) -> None : \"\"\"Counts the number of push ups. Args: img (np.ndarray): The image in each frame. img_size (Tuple[int, int]): The width and height of the image in each frame. the_keypoints (np.ndarray): The keypoints predicted in each frame. the_keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. \"\"\" interested_keypoints_names_to_index = { self . global_params_dataclass . KP_NAME_TO_INDEX [ interested_keypoint ]: interested_keypoint for interested_keypoint in self . interested_keypoints } self . reset_keypoints_to_none () for keypoint_idx , ( keypoints , keypoint_score ) in enumerate ( zip ( the_keypoints , the_keypoint_scores ) ): if keypoint_score >= self . keypoint_threshold : x , y = map_keypoint_to_image_coords ( keypoints . tolist (), img_size ) x_y_str = f \"( { x } , { y } )\" if keypoint_idx in interested_keypoints_names_to_index : keypoint_name = interested_keypoints_names_to_index [ keypoint_idx ] setattr ( self , keypoint_name , ( x , y )) the_color = self . global_params_dataclass . YELLOW else : the_color = self . global_params_dataclass . WHITE draw_text ( img , x_y_str , ( x , y ), color = the_color , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.4 , thickness = 2 , ) # all keypoints must be non-none if self . left_elbow and self . left_shoulder and self . left_wrist : left_elbow_angle = self . calculate_angle_using_dot_prod ( self . left_shoulder , self . left_elbow , self . left_wrist ) self . elbow_angle = left_elbow_angle # Check to ensure right form before starting the program if self . is_up_pose ( left_elbow_angle ): self . have_started_push_ups = True # Check for full range of motion for the pushup if self . have_started_push_ups : # the two if-statements are mutually exclusive: won't happen at the same time. if ( self . is_down_pose ( left_elbow_angle ) and self . expected_pose == \"down\" ): self . inc_num_push_ups () self . expected_pose = \"up\" if ( self . is_up_pose ( left_elbow_angle ) and self . expected_pose == \"up\" ): self . inc_num_push_ups () self . expected_pose = \"down\" pushup_str = f \"#push_ups = { self . num_push_ups } \" draw_text ( img , pushup_str , ( 20 , 30 ), color = self . global_params_dataclass . YELLOW , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1 , thickness = 3 , ) # pylint: disable=trailing-whitespace @staticmethod def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ) -> float : r \"\"\"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: $$ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} $$ Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Returns: angle (float): Angle between vectors BA and BC in radians or degrees. Shape: - Input: - a (np.ndarray): (2, ) - b (np.ndarray): (2, ) - c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) # arccos range is [0, pi] angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node draws keypoints and counts the number of push ups. Args: inputs (dict): Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. Note: To check the expected shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to [PeekingDuck API Documentation](https://peekingduck.readthedocs.io/en/stable/nodes/model.movenet.html#module-model.movenet). Returns: outputs (dict): Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. \"\"\" # get required inputs from pipeline img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] filename = inputs [ \"filename\" ] # image width, height img_size = ( img . shape [ 1 ], img . shape [ 0 ]) # frame count should not be in the if-clause self . frame_count += 1 if not self . is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ): # assume each frame has only one person; # note this bbox is from the posenet/movenet and not from yolo. the_bbox = bboxes [ 0 ] # bbox_scores are from yolo and not posenet/movenet. the_bbox_score = bbox_scores [ 0 ] if len ( bbox_scores ) > 0 else 0 x1 , _y1 , _x2 , y2 = map_bbox_to_image_coords ( the_bbox , img_size ) score_str = f \"BBox { the_bbox_score : 0.2f } \" # get bounding box confidence score and draw it at the left-bottom # (x1, y2) corner of the bounding box (offset by 30 pixels) draw_text ( img , score_str , ( x1 , y2 - 30 ), color = self . global_params_dataclass . WHITE , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1.0 , thickness = 3 , ) # assume each frame has only one person; the_keypoints = keypoints [ 0 ] the_keypoint_scores = keypoint_scores [ 0 ] # count the number of push ups self . count_push_ups ( img , img_size , the_keypoints , the_keypoint_scores ) # careful not to indent this return statement; # if the if-clause is false, then no dict will be returned and will crash the pipeline return { \"filename\" : filename , \"expected_pose\" : self . expected_pose , \"num_push_ups\" : self . num_push_ups , \"frame_count\" : self . frame_count , \"elbow_angle\" : self . elbow_angle , \"elbow_keypoint\" : self . left_elbow , \"shoulder_keypoint\" : self . left_shoulder , \"wrist_keypoint\" : self . left_wrist , } calculate_angle_using_dot_prod ( a , b , c , return_as_degrees = True ) staticmethod Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: \\[ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} \\] Parameters: Name Type Description Default a np.ndarray Point a corresponding to A. required b np.ndarray Point b corresponding to B. required c np.ndarray Point c corresponding to C. required return_as_degrees bool Returns angle in degrees if True else radians. Default: True. True Returns: Type Description angle (float) Angle between vectors BA and BC in radians or degrees. Shape Input: a (np.ndarray): (2, ) b (np.ndarray): (2, ) c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = ( 6 , 0 ) >>> b = ( 0 , 0 ) >>> c = ( 6 , 6 ) >>> calculate_angle_using_dot_prod ( a , b , c ) 45.0 Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py @staticmethod def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ) -> float : r \"\"\"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: $$ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} $$ Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Returns: angle (float): Angle between vectors BA and BC in radians or degrees. Shape: - Input: - a (np.ndarray): (2, ) - b (np.ndarray): (2, ) - c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) # arccos range is [0, pi] angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle count_push_ups ( self , img , img_size , the_keypoints , the_keypoint_scores ) Counts the number of push ups. Parameters: Name Type Description Default img np.ndarray The image in each frame. required img_size Tuple[int, int] The width and height of the image in each frame. required the_keypoints np.ndarray The keypoints predicted in each frame. required the_keypoint_scores np.ndarray The keypoint scores predicted in each frame. required Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def count_push_ups ( self , img : np . ndarray , img_size : Tuple [ int , int ], the_keypoints : np . ndarray , the_keypoint_scores : np . ndarray , ) -> None : \"\"\"Counts the number of push ups. Args: img (np.ndarray): The image in each frame. img_size (Tuple[int, int]): The width and height of the image in each frame. the_keypoints (np.ndarray): The keypoints predicted in each frame. the_keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. \"\"\" interested_keypoints_names_to_index = { self . global_params_dataclass . KP_NAME_TO_INDEX [ interested_keypoint ]: interested_keypoint for interested_keypoint in self . interested_keypoints } self . reset_keypoints_to_none () for keypoint_idx , ( keypoints , keypoint_score ) in enumerate ( zip ( the_keypoints , the_keypoint_scores ) ): if keypoint_score >= self . keypoint_threshold : x , y = map_keypoint_to_image_coords ( keypoints . tolist (), img_size ) x_y_str = f \"( { x } , { y } )\" if keypoint_idx in interested_keypoints_names_to_index : keypoint_name = interested_keypoints_names_to_index [ keypoint_idx ] setattr ( self , keypoint_name , ( x , y )) the_color = self . global_params_dataclass . YELLOW else : the_color = self . global_params_dataclass . WHITE draw_text ( img , x_y_str , ( x , y ), color = the_color , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.4 , thickness = 2 , ) # all keypoints must be non-none if self . left_elbow and self . left_shoulder and self . left_wrist : left_elbow_angle = self . calculate_angle_using_dot_prod ( self . left_shoulder , self . left_elbow , self . left_wrist ) self . elbow_angle = left_elbow_angle # Check to ensure right form before starting the program if self . is_up_pose ( left_elbow_angle ): self . have_started_push_ups = True # Check for full range of motion for the pushup if self . have_started_push_ups : # the two if-statements are mutually exclusive: won't happen at the same time. if ( self . is_down_pose ( left_elbow_angle ) and self . expected_pose == \"down\" ): self . inc_num_push_ups () self . expected_pose = \"up\" if ( self . is_up_pose ( left_elbow_angle ) and self . expected_pose == \"up\" ): self . inc_num_push_ups () self . expected_pose = \"down\" pushup_str = f \"#push_ups = { self . num_push_ups } \" draw_text ( img , pushup_str , ( 20 , 30 ), color = self . global_params_dataclass . YELLOW , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1 , thickness = 3 , ) inc_num_push_ups ( self ) Increments the number of push ups by 0.5 for every directional change. Returns: Type Description self.num_push_ups (float) Cumulative number of push ups. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def inc_num_push_ups ( self ) -> float : \"\"\"Increments the number of push ups by 0.5 for every directional change. Returns: self.num_push_ups (float): Cumulative number of push ups. \"\"\" self . num_push_ups += 0.5 return self . num_push_ups is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ) staticmethod Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Parameters: Name Type Description Default bboxes np.ndarray The bounding boxes. required keypoints np.ndarray The keypoints. required keypoint_scores np.ndarray The keypoint scores. required Returns: Type Description bool True if the bounding box or keypoints are empty, False otherwise. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py @staticmethod def is_bbox_or_keypoints_empty ( bboxes : np . ndarray , keypoints : np . ndarray , keypoint_scores : np . ndarray , ) -> bool : \"\"\"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Args: bboxes (np.ndarray): The bounding boxes. keypoints (np.ndarray): The keypoints. keypoint_scores (np.ndarray): The keypoint scores. Returns: bool: True if the bounding box or keypoints are empty, False otherwise. \"\"\" return ( len ( bboxes ) == 0 or len ( keypoints ) == 0 or len ( keypoint_scores ) == 0 ) is_down_pose ( self , elbow_angle ) Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Parameters: Name Type Description Default elbow_angle float The angle of the elbow. required Returns: Type Description bool True if the pose is a down pose, False otherwise. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def is_down_pose ( self , elbow_angle : float ) -> None : \"\"\"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a down pose, False otherwise. \"\"\" return ( elbow_angle <= self . push_up_pose_params_dataclass . ending_elbow_angle ) is_up_pose ( self , elbow_angle ) Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Parameters: Name Type Description Default elbow_angle float The angle of the elbow. required Returns: Type Description bool True if the pose is a up pose, False otherwise. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def is_up_pose ( self , elbow_angle : float ) -> bool : \"\"\"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a up pose, False otherwise. \"\"\" return ( elbow_angle > self . push_up_pose_params_dataclass . starting_elbow_angle ) reset_keypoints_to_none ( self ) Reset all keypoints attributes to None after each frame. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def reset_keypoints_to_none ( self ) -> None : \"\"\"Reset all keypoints attributes to None after each frame.\"\"\" for interested_keypoint in self . interested_keypoints : setattr ( self , interested_keypoint , None ) run ( self , inputs ) This node draws keypoints and counts the number of push ups. Parameters: Name Type Description Default inputs dict Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. required Note To check the expected shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to PeekingDuck API Documentation . Returns: Type Description outputs (dict) Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node draws keypoints and counts the number of push ups. Args: inputs (dict): Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. Note: To check the expected shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to [PeekingDuck API Documentation](https://peekingduck.readthedocs.io/en/stable/nodes/model.movenet.html#module-model.movenet). Returns: outputs (dict): Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. \"\"\" # get required inputs from pipeline img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] filename = inputs [ \"filename\" ] # image width, height img_size = ( img . shape [ 1 ], img . shape [ 0 ]) # frame count should not be in the if-clause self . frame_count += 1 if not self . is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ): # assume each frame has only one person; # note this bbox is from the posenet/movenet and not from yolo. the_bbox = bboxes [ 0 ] # bbox_scores are from yolo and not posenet/movenet. the_bbox_score = bbox_scores [ 0 ] if len ( bbox_scores ) > 0 else 0 x1 , _y1 , _x2 , y2 = map_bbox_to_image_coords ( the_bbox , img_size ) score_str = f \"BBox { the_bbox_score : 0.2f } \" # get bounding box confidence score and draw it at the left-bottom # (x1, y2) corner of the bounding box (offset by 30 pixels) draw_text ( img , score_str , ( x1 , y2 - 30 ), color = self . global_params_dataclass . WHITE , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1.0 , thickness = 3 , ) # assume each frame has only one person; the_keypoints = keypoints [ 0 ] the_keypoint_scores = keypoint_scores [ 0 ] # count the number of push ups self . count_push_ups ( img , img_size , the_keypoints , the_keypoint_scores ) # careful not to indent this return statement; # if the if-clause is false, then no dict will be returned and will crash the pipeline return { \"filename\" : filename , \"expected_pose\" : self . expected_pose , \"num_push_ups\" : self . num_push_ups , \"frame_count\" : self . frame_count , \"elbow_angle\" : self . elbow_angle , \"elbow_keypoint\" : self . left_elbow , \"shoulder_keypoint\" : self . left_shoulder , \"wrist_keypoint\" : self . left_wrist , } PushupPoseParams dataclass Push up pose parameters. Attributes: Name Type Description starting_elbow_angle float The threshold angle formed between the wrist, elbow and shoulder for starting (up) pose. ending_elbow_angle float The threshold angle formed between the wrist, elbow and shoulder for ending (down) pose. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py class PushupPoseParams : \"\"\"Push up pose parameters. Attributes: starting_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for starting (up) pose. ending_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for ending (down) pose. \"\"\" starting_elbow_angle : float ending_elbow_angle : float @classmethod def from_dict ( cls : Type [ \"PushupPoseParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"PushupPoseParams\" ]: \"\"\"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Note: One can also initialize the dataclass push_up_pose = PushupPoseParams(**params_dict). However this is assuming that params_dict only contain keys that are in the dataclass. May consider a method to get the keys of the dataclass to reduce typing out the keys below. [Reference](https://stackoverflow.com/questions/66499217/python-how-to-get-attributes-and-their-type-from-a-dataclass) Args: params_dict (Dict[str, Any]): Dictionary with keys that are in the dataclass. Returns: (PushupPoseParams): Dataclass with the parameters initalized from params_dict. \"\"\" return cls ( starting_elbow_angle = params_dict [ \"starting_elbow_angle\" ], ending_elbow_angle = params_dict [ \"ending_elbow_angle\" ], ) from_dict ( params_dict ) classmethod Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Note One can also initialize the dataclass push_up_pose = PushupPoseParams(**params_dict). However this is assuming that params_dict only contain keys that are in the dataclass. May consider a method to get the keys of the dataclass to reduce typing out the keys below. Reference Parameters: Name Type Description Default params_dict Dict[str, Any] Dictionary with keys that are in the dataclass. required Returns: Type Description (PushupPoseParams) Dataclass with the parameters initalized from params_dict. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py @classmethod def from_dict ( cls : Type [ \"PushupPoseParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"PushupPoseParams\" ]: \"\"\"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Note: One can also initialize the dataclass push_up_pose = PushupPoseParams(**params_dict). However this is assuming that params_dict only contain keys that are in the dataclass. May consider a method to get the keys of the dataclass to reduce typing out the keys below. [Reference](https://stackoverflow.com/questions/66499217/python-how-to-get-attributes-and-their-type-from-a-dataclass) Args: params_dict (Dict[str, Any]): Dictionary with keys that are in the dataclass. Returns: (PushupPoseParams): Dataclass with the parameters initalized from params_dict. \"\"\" return cls ( starting_elbow_angle = params_dict [ \"starting_elbow_angle\" ], ending_elbow_angle = params_dict [ \"ending_elbow_angle\" ], )","title":"Exercise Counter"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.GlobalParams","text":"Global parameters for the node. Attributes: Name Type Description FONT int Font for the text. WHITE int White color in BGR. YELLOW int Yellow color in BGR. KP_NAME_TO_INDEX Dict[str, int] PoseNet/MoveNet's skeletal keypoints name to index mapping. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py class GlobalParams : \"\"\"Global parameters for the node. Attributes: FONT (int): Font for the text. WHITE (int): White color in BGR. YELLOW (int): Yellow color in BGR. KP_NAME_TO_INDEX (Dict[str, int]): PoseNet/MoveNet's skeletal keypoints name to index mapping. \"\"\" FONT : int = cv2 . FONT_HERSHEY_SIMPLEX WHITE : Tuple [ int ] = ( 255 , 255 , 255 ) YELLOW : Tuple [ int ] = ( 0 , 255 , 255 ) KP_NAME_TO_INDEX : Dict [ str , int ] = field ( default_factory = lambda : { \"nose\" : 0 , \"left_eye\" : 1 , \"right_eye\" : 2 , \"left_ear\" : 3 , \"right_ear\" : 4 , \"left_shoulder\" : 5 , \"right_shoulder\" : 6 , \"left_elbow\" : 7 , \"right_elbow\" : 8 , \"left_wrist\" : 9 , \"right_wrist\" : 10 , \"left_hip\" : 11 , \"right_hip\" : 12 , \"left_knee\" : 13 , \"right_knee\" : 14 , \"left_ankle\" : 15 , \"right_ankle\" : 16 , } )","title":"GlobalParams"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node","text":"Custom node to display keypoints and count number of hand waves Parameters: Name Type Description Default config obj: Dict[str, Any] | :obj: None ): Node configuration. None config.exercise_name str Name of the exercise. Default: \"pushups\". required config.keypoint_threshold float Ignore keypoints below this threshold. Default: 0.3. required config.push_up_pose_params obj: Dict[str, Any] ): Parameters for the push up pose. Default: {starting_elbow_angle: 155, ending_elbow_angle: 90}. required Attributes: Name Type Description self.frame_count int Track the number of frames processed. self.expected_pose str The expected pose. Default: \"down\". self.num_push_ups float Cumulative number of push ups. Default: 0. self.have_started_push_ups bool Whether or not the push ups have started. Default: False. self.elbow_angle float Angle of the elbow. Default: None. self.global_params_dataclass GlobalParams Global parameters for the node. self.push_up_pose_params_dataclass PushupPoseParams Push up pose parameters. self.interested_keypoints List[str] List of keypoints to track. Default: [\"left_elbow\", \"left_shoulder\", \"left_wrist\"]. self.left_elbow float Keypoints of the left elbow. Default: None. self.left_shoulder float Keypoints of the left shoulder. Default: None. self.left_wrist float Keypoints of the left wrist. Default: None. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py class Node ( AbstractNode ): \"\"\"Custom node to display keypoints and count number of hand waves Args: config (:obj:`Dict[str, Any]` | :obj:`None`): Node configuration. config.exercise_name (str): Name of the exercise. Default: \"pushups\". config.keypoint_threshold (float): Ignore keypoints below this threshold. Default: 0.3. config.push_up_pose_params (:obj:`Dict[str, Any]`): Parameters for the push up pose. Default: {starting_elbow_angle: 155, ending_elbow_angle: 90}. Attributes: self.frame_count (int): Track the number of frames processed. self.expected_pose (str): The expected pose. Default: \"down\". self.num_push_ups (float): Cumulative number of push ups. Default: 0. self.have_started_push_ups (bool): Whether or not the push ups have started. Default: False. self.elbow_angle (float): Angle of the elbow. Default: None. self.global_params_dataclass (GlobalParams): Global parameters for the node. self.push_up_pose_params_dataclass (PushupPoseParams): Push up pose parameters. self.interested_keypoints (List[str]): List of keypoints to track. Default: [\"left_elbow\", \"left_shoulder\", \"left_wrist\"]. self.left_elbow (float): Keypoints of the left elbow. Default: None. self.left_shoulder (float): Keypoints of the left shoulder. Default: None. self.left_wrist (float): Keypoints of the left wrist. Default: None. \"\"\" def __init__ ( self , config : Dict [ str , Any ] = None , ** kwargs : Any ) -> None : super () . __init__ ( config , node_path = __name__ , ** kwargs ) self . logger = logging . getLogger ( __name__ ) self . exercise_name : str self . keypoint_threshold : float # ignore keypoints below this threshold self . push_up_pose_params : Dict [ str , Any ] self . logger . info ( f \"Initialize Exercise Type: { self . exercise_name } !\" ) self . frame_count = 0 self . expected_pose = \"down\" self . num_push_ups = 0 self . have_started_push_ups = False self . elbow_angle = None self . global_params_dataclass = GlobalParams () self . push_up_pose_params_dataclass = PushupPoseParams . from_dict ( self . push_up_pose_params ) self . interested_keypoints = [ \"left_elbow\" , \"left_shoulder\" , \"left_wrist\" , ] # each element in the self.interested_keypoints list will now become an attribute initialized to None self . reset_keypoints_to_none () def reset_keypoints_to_none ( self ) -> None : \"\"\"Reset all keypoints attributes to None after each frame.\"\"\" for interested_keypoint in self . interested_keypoints : setattr ( self , interested_keypoint , None ) def inc_num_push_ups ( self ) -> float : \"\"\"Increments the number of push ups by 0.5 for every directional change. Returns: self.num_push_ups (float): Cumulative number of push ups. \"\"\" self . num_push_ups += 0.5 return self . num_push_ups def is_up_pose ( self , elbow_angle : float ) -> bool : \"\"\"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a up pose, False otherwise. \"\"\" return ( elbow_angle > self . push_up_pose_params_dataclass . starting_elbow_angle ) def is_down_pose ( self , elbow_angle : float ) -> None : \"\"\"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a down pose, False otherwise. \"\"\" return ( elbow_angle <= self . push_up_pose_params_dataclass . ending_elbow_angle ) @staticmethod def is_bbox_or_keypoints_empty ( bboxes : np . ndarray , keypoints : np . ndarray , keypoint_scores : np . ndarray , ) -> bool : \"\"\"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Args: bboxes (np.ndarray): The bounding boxes. keypoints (np.ndarray): The keypoints. keypoint_scores (np.ndarray): The keypoint scores. Returns: bool: True if the bounding box or keypoints are empty, False otherwise. \"\"\" return ( len ( bboxes ) == 0 or len ( keypoints ) == 0 or len ( keypoint_scores ) == 0 ) # pylint: disable=too-many-locals def count_push_ups ( self , img : np . ndarray , img_size : Tuple [ int , int ], the_keypoints : np . ndarray , the_keypoint_scores : np . ndarray , ) -> None : \"\"\"Counts the number of push ups. Args: img (np.ndarray): The image in each frame. img_size (Tuple[int, int]): The width and height of the image in each frame. the_keypoints (np.ndarray): The keypoints predicted in each frame. the_keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. \"\"\" interested_keypoints_names_to_index = { self . global_params_dataclass . KP_NAME_TO_INDEX [ interested_keypoint ]: interested_keypoint for interested_keypoint in self . interested_keypoints } self . reset_keypoints_to_none () for keypoint_idx , ( keypoints , keypoint_score ) in enumerate ( zip ( the_keypoints , the_keypoint_scores ) ): if keypoint_score >= self . keypoint_threshold : x , y = map_keypoint_to_image_coords ( keypoints . tolist (), img_size ) x_y_str = f \"( { x } , { y } )\" if keypoint_idx in interested_keypoints_names_to_index : keypoint_name = interested_keypoints_names_to_index [ keypoint_idx ] setattr ( self , keypoint_name , ( x , y )) the_color = self . global_params_dataclass . YELLOW else : the_color = self . global_params_dataclass . WHITE draw_text ( img , x_y_str , ( x , y ), color = the_color , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.4 , thickness = 2 , ) # all keypoints must be non-none if self . left_elbow and self . left_shoulder and self . left_wrist : left_elbow_angle = self . calculate_angle_using_dot_prod ( self . left_shoulder , self . left_elbow , self . left_wrist ) self . elbow_angle = left_elbow_angle # Check to ensure right form before starting the program if self . is_up_pose ( left_elbow_angle ): self . have_started_push_ups = True # Check for full range of motion for the pushup if self . have_started_push_ups : # the two if-statements are mutually exclusive: won't happen at the same time. if ( self . is_down_pose ( left_elbow_angle ) and self . expected_pose == \"down\" ): self . inc_num_push_ups () self . expected_pose = \"up\" if ( self . is_up_pose ( left_elbow_angle ) and self . expected_pose == \"up\" ): self . inc_num_push_ups () self . expected_pose = \"down\" pushup_str = f \"#push_ups = { self . num_push_ups } \" draw_text ( img , pushup_str , ( 20 , 30 ), color = self . global_params_dataclass . YELLOW , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1 , thickness = 3 , ) # pylint: disable=trailing-whitespace @staticmethod def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ) -> float : r \"\"\"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: $$ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} $$ Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Returns: angle (float): Angle between vectors BA and BC in radians or degrees. Shape: - Input: - a (np.ndarray): (2, ) - b (np.ndarray): (2, ) - c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) # arccos range is [0, pi] angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node draws keypoints and counts the number of push ups. Args: inputs (dict): Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. Note: To check the expected shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to [PeekingDuck API Documentation](https://peekingduck.readthedocs.io/en/stable/nodes/model.movenet.html#module-model.movenet). Returns: outputs (dict): Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. \"\"\" # get required inputs from pipeline img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] filename = inputs [ \"filename\" ] # image width, height img_size = ( img . shape [ 1 ], img . shape [ 0 ]) # frame count should not be in the if-clause self . frame_count += 1 if not self . is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ): # assume each frame has only one person; # note this bbox is from the posenet/movenet and not from yolo. the_bbox = bboxes [ 0 ] # bbox_scores are from yolo and not posenet/movenet. the_bbox_score = bbox_scores [ 0 ] if len ( bbox_scores ) > 0 else 0 x1 , _y1 , _x2 , y2 = map_bbox_to_image_coords ( the_bbox , img_size ) score_str = f \"BBox { the_bbox_score : 0.2f } \" # get bounding box confidence score and draw it at the left-bottom # (x1, y2) corner of the bounding box (offset by 30 pixels) draw_text ( img , score_str , ( x1 , y2 - 30 ), color = self . global_params_dataclass . WHITE , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1.0 , thickness = 3 , ) # assume each frame has only one person; the_keypoints = keypoints [ 0 ] the_keypoint_scores = keypoint_scores [ 0 ] # count the number of push ups self . count_push_ups ( img , img_size , the_keypoints , the_keypoint_scores ) # careful not to indent this return statement; # if the if-clause is false, then no dict will be returned and will crash the pipeline return { \"filename\" : filename , \"expected_pose\" : self . expected_pose , \"num_push_ups\" : self . num_push_ups , \"frame_count\" : self . frame_count , \"elbow_angle\" : self . elbow_angle , \"elbow_keypoint\" : self . left_elbow , \"shoulder_keypoint\" : self . left_shoulder , \"wrist_keypoint\" : self . left_wrist , }","title":"Node"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.calculate_angle_using_dot_prod","text":"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: \\[ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} \\] Parameters: Name Type Description Default a np.ndarray Point a corresponding to A. required b np.ndarray Point b corresponding to B. required c np.ndarray Point c corresponding to C. required return_as_degrees bool Returns angle in degrees if True else radians. Default: True. True Returns: Type Description angle (float) Angle between vectors BA and BC in radians or degrees. Shape Input: a (np.ndarray): (2, ) b (np.ndarray): (2, ) c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = ( 6 , 0 ) >>> b = ( 0 , 0 ) >>> c = ( 6 , 6 ) >>> calculate_angle_using_dot_prod ( a , b , c ) 45.0 Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py @staticmethod def calculate_angle_using_dot_prod ( a : np . ndarray , b : np . ndarray , c : np . ndarray , return_as_degrees : bool = True , ) -> float : r \"\"\"Takes in three points and calculates the angle between them using dot product. Let B be the common point of two vectors BA and BC, then angle ABC is the angle between vectors BA and BC. This function calculates the angle ABC using the dot product formula: $$ \\begin{align*} BA &= a - b \\\\ BC &= c - b \\\\ \\cos(angle(ABC)) &= \\dfrac{(BA \\cdot BC)}{(|BA||BC|)} \\end{align*} $$ Args: a (np.ndarray): Point a corresponding to A. b (np.ndarray): Point b corresponding to B. c (np.ndarray): Point c corresponding to C. return_as_degrees (bool): Returns angle in degrees if True else radians. Default: True. Returns: angle (float): Angle between vectors BA and BC in radians or degrees. Shape: - Input: - a (np.ndarray): (2, ) - b (np.ndarray): (2, ) - c (np.ndarray): (2, ) Examples: >>> import numpy as np >>> a = (6, 0) >>> b = (0, 0) >>> c = (6, 6) >>> calculate_angle_using_dot_prod(a,b,c) 45.0 \"\"\" # turn the points into numpy arrays a = np . asarray ( a ) b = np . asarray ( b ) c = np . asarray ( c ) ba = a - b bc = c - b cosine_angle = np . dot ( ba , bc ) / ( np . linalg . norm ( ba ) * np . linalg . norm ( bc ) ) # arccos range is [0, pi] angle = np . arccos ( cosine_angle ) if return_as_degrees : return np . degrees ( angle ) return angle","title":"calculate_angle_using_dot_prod()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.count_push_ups","text":"Counts the number of push ups. Parameters: Name Type Description Default img np.ndarray The image in each frame. required img_size Tuple[int, int] The width and height of the image in each frame. required the_keypoints np.ndarray The keypoints predicted in each frame. required the_keypoint_scores np.ndarray The keypoint scores predicted in each frame. required Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def count_push_ups ( self , img : np . ndarray , img_size : Tuple [ int , int ], the_keypoints : np . ndarray , the_keypoint_scores : np . ndarray , ) -> None : \"\"\"Counts the number of push ups. Args: img (np.ndarray): The image in each frame. img_size (Tuple[int, int]): The width and height of the image in each frame. the_keypoints (np.ndarray): The keypoints predicted in each frame. the_keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. \"\"\" interested_keypoints_names_to_index = { self . global_params_dataclass . KP_NAME_TO_INDEX [ interested_keypoint ]: interested_keypoint for interested_keypoint in self . interested_keypoints } self . reset_keypoints_to_none () for keypoint_idx , ( keypoints , keypoint_score ) in enumerate ( zip ( the_keypoints , the_keypoint_scores ) ): if keypoint_score >= self . keypoint_threshold : x , y = map_keypoint_to_image_coords ( keypoints . tolist (), img_size ) x_y_str = f \"( { x } , { y } )\" if keypoint_idx in interested_keypoints_names_to_index : keypoint_name = interested_keypoints_names_to_index [ keypoint_idx ] setattr ( self , keypoint_name , ( x , y )) the_color = self . global_params_dataclass . YELLOW else : the_color = self . global_params_dataclass . WHITE draw_text ( img , x_y_str , ( x , y ), color = the_color , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 0.4 , thickness = 2 , ) # all keypoints must be non-none if self . left_elbow and self . left_shoulder and self . left_wrist : left_elbow_angle = self . calculate_angle_using_dot_prod ( self . left_shoulder , self . left_elbow , self . left_wrist ) self . elbow_angle = left_elbow_angle # Check to ensure right form before starting the program if self . is_up_pose ( left_elbow_angle ): self . have_started_push_ups = True # Check for full range of motion for the pushup if self . have_started_push_ups : # the two if-statements are mutually exclusive: won't happen at the same time. if ( self . is_down_pose ( left_elbow_angle ) and self . expected_pose == \"down\" ): self . inc_num_push_ups () self . expected_pose = \"up\" if ( self . is_up_pose ( left_elbow_angle ) and self . expected_pose == \"up\" ): self . inc_num_push_ups () self . expected_pose = \"down\" pushup_str = f \"#push_ups = { self . num_push_ups } \" draw_text ( img , pushup_str , ( 20 , 30 ), color = self . global_params_dataclass . YELLOW , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1 , thickness = 3 , )","title":"count_push_ups()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.inc_num_push_ups","text":"Increments the number of push ups by 0.5 for every directional change. Returns: Type Description self.num_push_ups (float) Cumulative number of push ups. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def inc_num_push_ups ( self ) -> float : \"\"\"Increments the number of push ups by 0.5 for every directional change. Returns: self.num_push_ups (float): Cumulative number of push ups. \"\"\" self . num_push_ups += 0.5 return self . num_push_ups","title":"inc_num_push_ups()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.is_bbox_or_keypoints_empty","text":"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Parameters: Name Type Description Default bboxes np.ndarray The bounding boxes. required keypoints np.ndarray The keypoints. required keypoint_scores np.ndarray The keypoint scores. required Returns: Type Description bool True if the bounding box or keypoints are empty, False otherwise. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py @staticmethod def is_bbox_or_keypoints_empty ( bboxes : np . ndarray , keypoints : np . ndarray , keypoint_scores : np . ndarray , ) -> bool : \"\"\"Checks if the bounding box or keypoints are empty. If any of them are empty, then we won't perform any further processing and go to next frame. Args: bboxes (np.ndarray): The bounding boxes. keypoints (np.ndarray): The keypoints. keypoint_scores (np.ndarray): The keypoint scores. Returns: bool: True if the bounding box or keypoints are empty, False otherwise. \"\"\" return ( len ( bboxes ) == 0 or len ( keypoints ) == 0 or len ( keypoint_scores ) == 0 )","title":"is_bbox_or_keypoints_empty()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.is_down_pose","text":"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Parameters: Name Type Description Default elbow_angle float The angle of the elbow. required Returns: Type Description bool True if the pose is a down pose, False otherwise. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def is_down_pose ( self , elbow_angle : float ) -> None : \"\"\"Checks if the pose is an \"down\" pose by checking if the elbow angle is more than the ending elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a down pose, False otherwise. \"\"\" return ( elbow_angle <= self . push_up_pose_params_dataclass . ending_elbow_angle )","title":"is_down_pose()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.is_up_pose","text":"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Parameters: Name Type Description Default elbow_angle float The angle of the elbow. required Returns: Type Description bool True if the pose is a up pose, False otherwise. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def is_up_pose ( self , elbow_angle : float ) -> bool : \"\"\"Checks if the pose is an \"up\" pose by checking if the elbow angle is more than the starting elbow angle threshold. Elbow angle is defined by connecting the wrist to the elbow to the shoulder. Args: elbow_angle (float): The angle of the elbow. Returns: bool: True if the pose is a up pose, False otherwise. \"\"\" return ( elbow_angle > self . push_up_pose_params_dataclass . starting_elbow_angle )","title":"is_up_pose()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.reset_keypoints_to_none","text":"Reset all keypoints attributes to None after each frame. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def reset_keypoints_to_none ( self ) -> None : \"\"\"Reset all keypoints attributes to None after each frame.\"\"\" for interested_keypoint in self . interested_keypoints : setattr ( self , interested_keypoint , None )","title":"reset_keypoints_to_none()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.Node.run","text":"This node draws keypoints and counts the number of push ups. Parameters: Name Type Description Default inputs dict Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. required Note To check the expected shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to PeekingDuck API Documentation . Returns: Type Description outputs (dict) Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py def run ( self , inputs : Dict [ str , Any ]) -> Dict [ str , Any ]: # type: ignore \"\"\"This node draws keypoints and counts the number of push ups. Args: inputs (dict): Dictionary with keys - img (np.ndarray): The image in each frame. - bboxes (np.ndarray): The bounding boxes predicted in each frame. Note this belongs to MoveNet. - bbox_scores (np.ndarray): The bounding box scores predicted in each frame. Note this belongs to Yolo. - keypoints (np.ndarray): The keypoints predicted in each frame. - keypoint_scores (np.ndarray): The keypoint scores predicted in each frame. - filename (str): The filename of the image/video. Note: To check the expected shapes of bboxes, bbox_scores, keypoints, and keypoint_scores, please refer to [PeekingDuck API Documentation](https://peekingduck.readthedocs.io/en/stable/nodes/model.movenet.html#module-model.movenet). Returns: outputs (dict): Dictionary with keys - filename: The filename of the image/video. - expected_pose: The expected pose of the image/video. - num_push_ups: The number of cumulative push ups. - frame_count: The number of frames processed. - elbow_angle: The angle between the wrist, elbow and shoulder. - elbow_keypoint: The keypoint coordinates of the elbow. - shoulder_keypoint: The keypoint coordinates of the shoulder. - wrist_keypoint: The keypoint coordinates of the wrist. \"\"\" # get required inputs from pipeline img = inputs [ \"img\" ] bboxes = inputs [ \"bboxes\" ] bbox_scores = inputs [ \"bbox_scores\" ] keypoints = inputs [ \"keypoints\" ] keypoint_scores = inputs [ \"keypoint_scores\" ] filename = inputs [ \"filename\" ] # image width, height img_size = ( img . shape [ 1 ], img . shape [ 0 ]) # frame count should not be in the if-clause self . frame_count += 1 if not self . is_bbox_or_keypoints_empty ( bboxes , keypoints , keypoint_scores ): # assume each frame has only one person; # note this bbox is from the posenet/movenet and not from yolo. the_bbox = bboxes [ 0 ] # bbox_scores are from yolo and not posenet/movenet. the_bbox_score = bbox_scores [ 0 ] if len ( bbox_scores ) > 0 else 0 x1 , _y1 , _x2 , y2 = map_bbox_to_image_coords ( the_bbox , img_size ) score_str = f \"BBox { the_bbox_score : 0.2f } \" # get bounding box confidence score and draw it at the left-bottom # (x1, y2) corner of the bounding box (offset by 30 pixels) draw_text ( img , score_str , ( x1 , y2 - 30 ), color = self . global_params_dataclass . WHITE , fontFace = cv2 . FONT_HERSHEY_SIMPLEX , fontScale = 1.0 , thickness = 3 , ) # assume each frame has only one person; the_keypoints = keypoints [ 0 ] the_keypoint_scores = keypoint_scores [ 0 ] # count the number of push ups self . count_push_ups ( img , img_size , the_keypoints , the_keypoint_scores ) # careful not to indent this return statement; # if the if-clause is false, then no dict will be returned and will crash the pipeline return { \"filename\" : filename , \"expected_pose\" : self . expected_pose , \"num_push_ups\" : self . num_push_ups , \"frame_count\" : self . frame_count , \"elbow_angle\" : self . elbow_angle , \"elbow_keypoint\" : self . left_elbow , \"shoulder_keypoint\" : self . left_shoulder , \"wrist_keypoint\" : self . left_wrist , }","title":"run()"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.PushupPoseParams","text":"Push up pose parameters. Attributes: Name Type Description starting_elbow_angle float The threshold angle formed between the wrist, elbow and shoulder for starting (up) pose. ending_elbow_angle float The threshold angle formed between the wrist, elbow and shoulder for ending (down) pose. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py class PushupPoseParams : \"\"\"Push up pose parameters. Attributes: starting_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for starting (up) pose. ending_elbow_angle (float): The threshold angle formed between the wrist, elbow and shoulder for ending (down) pose. \"\"\" starting_elbow_angle : float ending_elbow_angle : float @classmethod def from_dict ( cls : Type [ \"PushupPoseParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"PushupPoseParams\" ]: \"\"\"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Note: One can also initialize the dataclass push_up_pose = PushupPoseParams(**params_dict). However this is assuming that params_dict only contain keys that are in the dataclass. May consider a method to get the keys of the dataclass to reduce typing out the keys below. [Reference](https://stackoverflow.com/questions/66499217/python-how-to-get-attributes-and-their-type-from-a-dataclass) Args: params_dict (Dict[str, Any]): Dictionary with keys that are in the dataclass. Returns: (PushupPoseParams): Dataclass with the parameters initalized from params_dict. \"\"\" return cls ( starting_elbow_angle = params_dict [ \"starting_elbow_angle\" ], ending_elbow_angle = params_dict [ \"ending_elbow_angle\" ], )","title":"PushupPoseParams"},{"location":"api/exercise_counter_api/#custom_hn_exercise_counter.src.custom_nodes.dabble.exercise_counter.PushupPoseParams.from_dict","text":"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Note One can also initialize the dataclass push_up_pose = PushupPoseParams(**params_dict). However this is assuming that params_dict only contain keys that are in the dataclass. May consider a method to get the keys of the dataclass to reduce typing out the keys below. Reference Parameters: Name Type Description Default params_dict Dict[str, Any] Dictionary with keys that are in the dataclass. required Returns: Type Description (PushupPoseParams) Dataclass with the parameters initalized from params_dict. Source code in custom_hn_exercise_counter/src/custom_nodes/dabble/exercise_counter.py @classmethod def from_dict ( cls : Type [ \"PushupPoseParams\" ], params_dict : Dict [ str , Any ] ) -> Type [ \"PushupPoseParams\" ]: \"\"\"Takes in a dictionary of parameters and returns a PushupPoseParams Dataclass. Note: One can also initialize the dataclass push_up_pose = PushupPoseParams(**params_dict). However this is assuming that params_dict only contain keys that are in the dataclass. May consider a method to get the keys of the dataclass to reduce typing out the keys below. [Reference](https://stackoverflow.com/questions/66499217/python-how-to-get-attributes-and-their-type-from-a-dataclass) Args: params_dict (Dict[str, Any]): Dictionary with keys that are in the dataclass. Returns: (PushupPoseParams): Dataclass with the parameters initalized from params_dict. \"\"\" return cls ( starting_elbow_angle = params_dict [ \"starting_elbow_angle\" ], ending_elbow_angle = params_dict [ \"ending_elbow_angle\" ], )","title":"from_dict()"}]}